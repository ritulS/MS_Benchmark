{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs: traces_dict, node_details_dict and trace_details_dict\n",
    "# Node details dict= nid: [nis, type]\n",
    "### Config file: DB split and SLtype split\n",
    "### Outputs: updated_node_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dict(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        T_prime = pickle.load(pkl_file)\n",
    "    return T_prime\n",
    "\n",
    "def save_dict_as_pkl(traces_dict, file_name):\n",
    "    with open(file_name+'.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(traces_dict, pkl_file)\n",
    "\n",
    "def save_dict_as_json(traces_dict, file_name):\n",
    "    with open(file_name+'.json', 'w') as json_file:\n",
    "        json.dump(traces_dict, json_file)\n",
    "\n",
    "def read_yaml(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "def build_digraph_from_tracesdict(traces_dict):\n",
    "\n",
    "    full_graph_edge_list = []\n",
    "    for edge_list in traces_dict.values():\n",
    "        full_graph_edge_list.extend(edge_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(full_graph_edge_list)\n",
    "\n",
    "    return G\n",
    "\n",
    "def prune_node_details(traces_dict, node_dets):\n",
    "    nodes_from_traces = []\n",
    "    for _, e_list in traces_dict.items():\n",
    "        for e in e_list:\n",
    "            if e[0] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[0])\n",
    "            if e[1] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[1])\n",
    "    pruned_node_dets = {node: details for node, details\\\n",
    "                         in node_dets.items() if node in nodes_from_traces}\n",
    "\n",
    "    return pruned_node_dets\n",
    "\n",
    "def calc_graph_depth(G, initial_node):\n",
    "    def dfs(node, visited, stack):\n",
    "        # Using stack to avoid cycles\n",
    "        if node in stack:\n",
    "            return 0\n",
    "        if node in visited:\n",
    "            return visited[node]\n",
    "        stack.add(node)\n",
    "        max_depth = 0\n",
    "        for neighbour in G.successors(node):\n",
    "            depth = dfs(neighbour, visited, stack)\n",
    "            max_depth = max(max_depth, depth)\n",
    "        stack.remove(node)\n",
    "        visited[node] = max_depth + 1\n",
    "        \n",
    "        return visited[node]\n",
    "    \n",
    "    visited = {}\n",
    "    stack = set()\n",
    "\n",
    "    return dfs(initial_node, visited, stack)\n",
    "def find_inode_and_graph_depth(G):\n",
    "    max_depth = -1\n",
    "    node_with_max_depth = ''\n",
    "\n",
    "    for node in G.nodes():\n",
    "        depth = calc_graph_depth(G, node)\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "            node_with_max_depth = node\n",
    "    return node_with_max_depth, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes in node_dets:  446\n",
      "Number of SF nodes in trace graph: 135\n",
      "Number of SL nodes in trace graph: 311\n",
      "Database split Input: [['MongoDB', 10], ['Redis', 80], ['Postgres', 10]]\n",
      "Database split output: [['MongoDB', 13], ['Redis', 108], ['Postgres', 14]]\n",
      "Nodes Split Output: {'sf_split': {'MongoDB': {'count': 13, 'nodes_list': ['n355', 'n6761', 'n5358', 'n6456', 'n5288', 'n3125', 'n1624', 'n5124', 'n350', 'n7006', 'n4282', 'n7709', 'n7614']}, 'Redis': {'count': 108, 'nodes_list': ['n624', 'n7264', 'n2382', 'n2800', 'n2154', 'n3442', 'n7601', 'n6438', 'n7361', 'n33', 'n6578', 'n1063', 'n2695', 'n4895', 'n29', 'n1791', 'n2915', 'n1122', 'n4037', 'n2054', 'n2127', 'n3843', 'n4393', 'n6717', 'n7028', 'n25', 'n3882', 'n7049', 'n2006', 'n6818', 'n3428', 'n7957', 'n6941', 'n5913', 'n7529', 'n988', 'n2996', 'n7905', 'n7921', 'n3340', 'n5577', 'n870', 'n3814', 'n5661', 'n847', 'n8006', 'n1069', 'n7074', 'n377', 'n662', 'n6554', 'n4707', 'n5507', 'n6260', 'n607', 'n1702', 'n922', 'n4672', 'n7904', 'n7822', 'n3836', 'n5146', 'n4818', 'n7036', 'n6572', 'n4936', 'n6256', 'n2162', 'n6794', 'n7699', 'n6733', 'n1485', 'n4997', 'n530', 'n1649', 'n2841', 'n6470', 'n6621', 'n3510', 'n55', 'n1010', 'n3715', 'n4934', 'n4378', 'n5565', 'n5411', 'n2566', 'n6684', 'n3053', 'n4226', 'n7286', 'n5904', 'n5223', 'n2416', 'n310', 'n4655', 'n1408', 'n7451', 'n6475', 'n1194', 'n5528', 'n6801', 'n7732', 'n2025', 'n2839', 'n7592', 'n216', 'n2405']}, 'Postgres': {'count': 14, 'nodes_list': ['n2687', 'n125', 'n5024', 'n6781', 'n1347', 'n7088', 'n3894', 'n3496', 'n1749', 'n4365', 'n8', 'n3054', 'n2954', 'n3823']}}, 'sl_split': {'Python': {'count': 311, 'nodes_list': ['n1242', 'n5503', 'n2227', 'n1472', 'n5122', 'n2518', 'n4641', 'n7976', 'n385', 'n1039', 'n2041', 'n4760', 'n1098', 'n1094', 'n6759', 'n5977', 'n676', 'n16', 'n1919', 'n3007', 'n5665', 'n5205', 'n4996', 'n3062', 'n4100', 'n699', 'n1394', 'n2636', 'n1214', 'n6659', 'n5620', 'n2178', 'n5403', 'n3976', 'n6505', 'n1720', 'n8009', 'n1345', 'n7180', 'n5056', 'n5372', 'n6662', 'n1147', 'n371', 'n7883', 'n2780', 'n7381', 'n7525', 'n1299', 'n643', 'n7420', 'n5103', 'n5894', 'n4433', 'n4425', 'n6763', 'n4400', 'n1361', 'n7727', 'n7295', 'n52', 'n3609', 'n3389', 'n6688', 'n3302', 'n7825', 'n2257', 'n4897', 'n2853', 'n3033', 'n6493', 'n6392', 'n6624', 'n13', 'n5145', 'n7635', 'n3141', 'n7895', 'n7824', 'n5043', 'n7147', 'n4210', 'n6091', 'n3599', 'n3274', 'n2725', 'n5941', 'n2978', 'n2617', 'n127', 'n1311', 'n2439', 'n1016', 'n2487', 'n2789', 'n4175', 'n3872', 'n1466', 'n4767', 'n7753', 'n2609', 'n1571', 'n6664', 'n951', 'n4249', 'n4468', 'n2655', 'n3064', 'n4444', 'n5933', 'n7137', 'n4900', 'n1187', 'n6054', 'n6263', 'n3089', 'n7477', 'n6077', 'n7916', 'n1716', 'n2081', 'n2266', 'n3504', 'n4332', 'n6292', 'n2381', 'n2635', 'n816', 'n1570', 'n7197', 'n2533', 'n4052', 'n6278', 'n6693', 'n5830', 'n3441', 'n1767', 'n1627', 'n298', 'n2212', 'n1249', 'n6890', 'n4478', 'n1913', 'n2364', 'n4510', 'n6970', 'n6347', 'n6279', 'n6769', 'n3943', 'n4651', 'n7054', 'n3573', 'n3480', 'n5038', 'n208', 'n6343', 'n6924', 'n2369', 'n3100', 'n3124', 'n2163', 'n4820', 'n7929', 'n5469', 'n1961', 'n5435', 'n644', 'n4709', 'n1837', 'n1901', 'n3978', 'n3230', 'n3101', 'n2050', 'n964', 'n4406', 'n2532', 'n7408', 'n7457', 'n841', 'n2937', 'n6810', 'n6827', 'n3264', 'n2269', 'n905', 'n5105', 'n6149', 'n5346', 'n6909', 'n3507', 'n4845', 'n2956', 'n84', 'n805', 'n1967', 'n4275', 'n6573', 'n1301', 'n7611', 'n1865', 'n4240', 'n5076', 'n4778', 'n5991', 'n3241', 'n1835', 'n2464', 'n236', 'n2506', 'n3040', 'n5466', 'n3819', 'n32', 'n2157', 'n1365', 'n3488', 'n492', 'n5794', 'n5339', 'n3611', 'n739', 'n5027', 'n556', 'n2671', 'n6499', 'n7358', 'n1450', 'n223', 'n7907', 'n7559', 'n796', 'n7674', 'n7946', 'n3986', 'n1279', 'n5886', 'n3856', 'n3434', 'n1672', 'n6768', 'n3158', 'n234', 'n2267', 'n386', 'n6936', 'n1806', 'n2902', 'n1523', 'n6058', 'n6399', 'n7670', 'n3190', 'n289', 'n5329', 'n7567', 'n1465', 'n281', 'n617', 'n3897', 'n2949', 'n1957', 'n4779', 'n3052', 'n4347', 'n4771', 'n6491', 'n3088', 'n2187', 'n627', 'n215', 'n326', 'n192', 'n178', 'n976', 'n31', 'n7823', 'n7789', 'n1988', 'n5531', 'n2515', 'n1519', 'n6943', 'n817', 'n3857', 'n7597', 'n7730', 'n288', 'n2378', 'n7113', 'n7185', 'n5443', 'n6634', 'n516', 'n2679', 'n5000', 'n2226', 'n301', 'n4373', 'n513', 'n7207', 'n4821', 'n2003', 'n6915', 'n6959', 'n117', 'n591', 'n2821', 'n1410']}}}\n"
     ]
    }
   ],
   "source": [
    "# Read in configs\n",
    "config = read_yaml('enrichment_config.yaml')\n",
    "databases = config['Databases']\n",
    "workload_name = config['ExpWorkloadName']\n",
    "\n",
    "'''\n",
    "NODE ENRICHMENT\n",
    "---------------\n",
    "Input: Traces_dict, Node_details_dict\n",
    "Output: Node split output\n",
    "        node_split_output = {'sf_split': {DB1: {'count': 30, 'nodes_list': [nid1, nid2, ...]}, ...},..}\n",
    "                             'sl_split': ,,}\n",
    "'''\n",
    "# Node details dict= nid: [nis, SF, DB_name] (or) [nis, SL, SL_type]\n",
    "traces_dict = pkl_to_dict('traces/exp_500nodes_100ktraces.pkl')\n",
    "# selected_keys = ['0b66f86d15919401842041000d5482'] # Uncomment only for testing\n",
    "# traces_dict = {key: traces_dict[key] for key in selected_keys if key in traces_dict}\n",
    "\n",
    "node_dets = pkl_to_dict('node_and_trace_details/new_node_details_data.pkl')\n",
    "node_dets = prune_node_details(traces_dict, node_dets)\n",
    "print(\"Num nodes in node_dets: \", len(node_dets))\n",
    "\n",
    "sf_arr = [nid for nid, n_info in node_dets.items() if n_info[1] == \"db\"]\n",
    "sl_arr = [nid for nid, n_info in node_dets.items() if n_info[1] != \"db\"]\n",
    "\n",
    "sf_count = len(sf_arr)\n",
    "print(\"Number of SF nodes in trace graph:\", sf_count)\n",
    "sl_count = len(sl_arr)\n",
    "print(\"Number of SL nodes in trace graph:\", sl_count)\n",
    "total_nodes = sf_count + sl_count\n",
    "\n",
    "db_split_arr = [[db_name, info['percentage']] for db_name, info in databases.items()]# [[DB1, 30%],...]\n",
    "sl_type_split = [['Python', sl_count]]\n",
    "print(\"Database split Input:\", db_split_arr)\n",
    "\n",
    "def percent_to_count(arr, count):\n",
    "    raw_counts = [round(count * (i[1] / 100)) for i in arr]\n",
    "    diff = count - sum(raw_counts)\n",
    "    \n",
    "    # Distribute the difference\n",
    "    idx = 0\n",
    "    while diff != 0:\n",
    "        if diff > 0:\n",
    "            raw_counts[idx] += 1  # Increase by 1 if we need to add\n",
    "            diff -= 1\n",
    "        elif diff < 0:\n",
    "            raw_counts[idx] -= 1  # Decrease by 1 if we need to remove\n",
    "            diff += 1\n",
    "        idx = (idx + 1) % len(raw_counts)\n",
    "    \n",
    "    for idx, i in enumerate(arr):\n",
    "        arr[idx] = [i[0], raw_counts[idx]]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "db_split_arr = percent_to_count(db_split_arr, sf_count) # nid: [nis, SF, DB_name]\n",
    "print(\"Database split output:\", db_split_arr)\n",
    "\n",
    "\n",
    "sf_split_info = {ntype: {\"count\": value, \"nodes_list\": []} for ntype, value in db_split_arr}\n",
    "sl_split_info = {'Python': {\"count\": sl_count, \"nodes_list\": []}}\n",
    "\n",
    "def assign_nodes_to_types(split_arr, sfsl_arr, split_info):\n",
    "    sfsl_arr_cpy = sfsl_arr.copy()\n",
    "    # Assign nodes to db and sl types\n",
    "    for i in split_arr:\n",
    "        ctr = 0\n",
    "        name = i[0] # type name: eg: Mongo, Redis, Relay\n",
    "        for _ in range(i[1]):\n",
    "            ctr += 1\n",
    "            nid = sfsl_arr_cpy.pop(random.randint(0, len(sfsl_arr_cpy) - 1))\n",
    "            node_dets[nid].append(name) # add type to node details\n",
    "            split_info[name][\"nodes_list\"].append(nid) # add node to list of nodes for that type\n",
    "        # print(ctr, name)\n",
    "    return node_dets, split_info\n",
    "\n",
    "node_dets, sf_split_info = assign_nodes_to_types(db_split_arr, sf_arr, sf_split_info)\n",
    "node_dets, sl_split_info = assign_nodes_to_types(sl_type_split, sl_arr, sl_split_info)\n",
    "\n",
    "# Saving node split output\n",
    "node_split_output = {'sf_split': sf_split_info, 'sl_split': sl_split_info}\n",
    "print(\"Nodes Split Output:\", node_split_output)\n",
    "save_dict_as_json(node_split_output, f'enrichment_runs/{workload_name}/node_split_output')\n",
    "# save_dict_as_json(node_split_output, f'enrichment_runs/as_dmix1_2/node_split_output') ################# AS EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0b5218f615919497680352000ed6c1 1 5: Mongo testing\n",
    "\n",
    "## Sl to Sl: 0b51062f15919594922205000eaed6 (2 nodes)\n",
    "## Sl to Db: 0b66f86d15919401842041000d5482 (2 nodes)\n",
    "\n",
    "# ticker = 0\n",
    "# for tid in traces_dict:\n",
    "#     e_list = traces_dict[tid]\n",
    "#     sf_needed = 0\n",
    "#     t_nodes = []\n",
    "#     for e in e_list:\n",
    "#         if e[1] not in t_nodes:\n",
    "#             t_nodes.append(e[1])\n",
    "#         if e[0] not in t_nodes:\n",
    "#             t_nodes.append(e[0])\n",
    "#         if e[1] in sf_arr:\n",
    "#             sf_needed += 1\n",
    "#     # if sf_needed == 2 and len(t_nodes) < 35:\n",
    "#     #     ticker += 1\n",
    "#     #     print(tid, sf_needed, len(t_nodes))\n",
    "#     if sf_needed == 1 and len(t_nodes) < 35:\n",
    "#         ticker += 1\n",
    "#         print(tid, sf_needed, len(t_nodes))\n",
    "#         if ticker == 5:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle Ctr:  13489\n",
      "Num Unique nodes:  0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Object id Enrichment\n",
    "Output: Trace packets.\n",
    "        Trace packets = [t_node_calls_dict, t_data_ops_dict]\n",
    "        t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "        t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "\n",
    "class Wl_config:\n",
    "    \"\"\"\n",
    "    Format: record_count, record_size_dist,\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "    \"\"\"\n",
    "    def __init__(self, record_count, record_size_dist,\\\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed):\n",
    "        self.record_count = record_count\n",
    "        self.record_size_dist = record_size_dist\n",
    "        self.data_access_pattern = data_access_pattern\n",
    "        self.rw_ratio = rw_ratio\n",
    "        self.async_sync_ratio = async_sync_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Setting seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        # Generate object sizes and data access pattern\n",
    "        self.obj_ids_list = np.arange(1, self.record_count + 1)\n",
    "        self.object_sizes_dict = self.generate_object_sizes()\n",
    "        self.probabilities = self.generate_data_access_pattern()\n",
    "\n",
    "    def generate_object_sizes(self):\n",
    "        if self.record_size_dist == 'lognormal':\n",
    "            obj_sizes = np.random.lognormal(mean=np.log(self.record_count), \\\n",
    "                                                 sigma=np.log(self.record_count), \\\n",
    "                                                 size=self.record_count)\n",
    "        elif self.record_size_dist == 'uniform':\n",
    "            obj_sizes = np.random.uniform(low=1, high=self.record_count, size=self.record_count)\n",
    "        else:\n",
    "            raise ValueError('Invalid record size distribution, only lognormal & uniform are allowed for now')\n",
    "        return dict(zip(self.obj_ids_list, obj_sizes))\n",
    "    \n",
    "    def generate_data_access_pattern(self):\n",
    "        if self.data_access_pattern == 'zipfian':\n",
    "            alpha = 1.2\n",
    "            probabilities = np.random.zipf(alpha, len(self.obj_ids_list))\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        elif self.data_access_pattern == 'uniform':\n",
    "            probabilities = np.ones(len(self.obj_ids_list)) / len(self.obj_ids_list)\n",
    "        else:\n",
    "            raise ValueError('Invalid data access pattern, only zipfian & uniform are allowed for now.')\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "def gen_sfnode_dataops(sf_node, wl_config, traces_dict, node_dets):\n",
    "    '''\n",
    "    For a given sf node, generate data ops (count total dm calls to sf node)\n",
    "    Return: ops_dict= Key: op_id, Value: op_packet\n",
    "    op_packet = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': op_obj_id,\\\n",
    "                 'db': sf_node_db}\n",
    "    '''\n",
    "    obj_ids_list = wl_config.obj_ids_list\n",
    "    # obj_sizes_dict = wl_config.object_sizes_dict\n",
    "    data_acc_probabilities = wl_config.probabilities\n",
    "    w_prob = wl_config.rw_ratio / (1 + wl_config.rw_ratio)\n",
    "\n",
    "    sf_node_db = node_dets[sf_node][2]\n",
    "\n",
    "   # find the number of ops to be generated\n",
    "    total_ops = 0\n",
    "    for e_list in traces_dict.values():# count total dm calls to sf node\n",
    "        for e in e_list:\n",
    "            if e[1] == node:\n",
    "                total_ops += 1\n",
    "\n",
    "    # generate ops for sf node\n",
    "    ops_dict = {}   # key: op_id, value: op_packet\n",
    "    for op_id in range(1, total_ops + 1):\n",
    "        op_type = 'write' if random.random() < w_prob else 'read'\n",
    "        # op_obj_id = np.random.choice(obj_ids_list,\\\n",
    "        #                              p=data_acc_probabilities)# Select by data access pattern\n",
    "        op_obj_id = random.randrange(1, wl_config.record_count + 1)\n",
    "        # op_obj_size = obj_sizes_dict[op_obj_id]\n",
    "        operation = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': f\"key_{op_obj_id}\",\\\n",
    "                      'db': sf_node_db} # op_packet\n",
    "        ops_dict[op_id] = operation\n",
    "    \n",
    "    return ops_dict\n",
    "\n",
    "\n",
    "# convert edges_list to node_calls_dict format \n",
    "def gen_node_calls_dict(edges_list, async_sync_ratio):\n",
    "    '''\n",
    "    Return: node_calls_dict = Key: dm node, Value: list of [dm node, op_id, async_flag]\n",
    "            (op_id = -1 for SL) (async_flag = 1 for async, 0 for sync)\n",
    "    '''\n",
    "    node_calls_dict = {}\n",
    "    for edge in edges_list:\n",
    "        if edge[0] not in node_calls_dict:\n",
    "            node_calls_dict[edge[0]] = []\n",
    "        async_prob = async_sync_ratio / (1 + async_sync_ratio)\n",
    "        async_flag = 1 if random.random() < async_prob else 0\n",
    "        node_calls_dict[edge[0]].append([edge[1], -1, async_flag]) # [dm node, op_id, async/sync] (-1 for SL) (1/0 for async/sync)\n",
    "    return node_calls_dict\n",
    "\n",
    "\n",
    "# Reading enrichment config file\n",
    "enrichment_config = read_yaml('enrichment_config.yaml')\n",
    "record_count = enrichment_config['WorkloadConfig']['record_count']\n",
    "record_size_dist = enrichment_config['WorkloadConfig']['record_size_dist']\n",
    "data_access_pattern = enrichment_config['WorkloadConfig']['data_access_pattern']\n",
    "rw_ratio = enrichment_config['WorkloadConfig']['rw_ratio']\n",
    "async_sync_ratio = enrichment_config['WorkloadConfig']['async_sync_ratio']\n",
    "# Format: record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "wl1 = Wl_config(record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed=50) # to be read from config file\n",
    "\n",
    "# Generate data op packets for each sf node\n",
    "G_agg = build_digraph_from_tracesdict(traces_dict)\n",
    "overall_data_ops = {}   # key: sf_node, value: ops_dict\n",
    "check = 0\n",
    "for node in node_dets:\n",
    "    if node in G_agg.nodes() and node_dets[node][1] == 'db':\n",
    "        overall_data_ops[node] = \\\n",
    "            gen_sfnode_dataops(node, wl1, traces_dict, node_dets)\n",
    "\n",
    "def get_pop_first_dict_item(d):\n",
    "    first_key = list(d.keys())[0]\n",
    "    first_item = d.pop(first_key)\n",
    "    return first_key, first_item\n",
    "\n",
    "def get_node_type(node_id, data):\n",
    "    '''\n",
    "    data: node_split_output.json\n",
    "    '''\n",
    "    for split_type, services in data.items():\n",
    "        for service, service_data in services.items():\n",
    "            if node_id in service_data['nodes_list']:\n",
    "                return service\n",
    "\n",
    "def remove_self_node_calls(node_call_dict):\n",
    "    for node, dm_nodes in node_call_dict.items():\n",
    "        node_call_dict[node] = [dm_node for dm_node in dm_nodes if dm_node[0] != node]\n",
    "    return node_call_dict\n",
    "\n",
    "def get_leaf_nodes(node_call_dict):\n",
    "    '''Returns: leaf nodes in a request call graph'''\n",
    "    all_nodes = set(node_call_dict.keys())\n",
    "    called_nodes = set()\n",
    "    for calls in node_call_dict.values():\n",
    "        for call in calls:\n",
    "            called_nodes.add(call[0])\n",
    "    leaf_nodes = called_nodes - all_nodes\n",
    "    return leaf_nodes\n",
    "\n",
    "def get_logger_nodes_for_request_call_graph(node_call_dict):\n",
    "    '''Returns: list of nodes that log for the request call graph\n",
    "                SL leaf nodes and SL node predecessor to SF leaf nodes.\n",
    "    '''\n",
    "    logger_nodes = set()\n",
    "    t_leaf_nodes = get_leaf_nodes(node_call_dict) # find all leaf nodes\n",
    "    for ln in t_leaf_nodes:\n",
    "        for node, calls in node_call_dict.items():\n",
    "            for call in calls:\n",
    "                if call[0] == ln and call[1] != -1: # Leaf SF node\n",
    "                    logger_nodes.add(node)\n",
    "                elif call[0] == ln and call[1] == -1: # Leaf SL node\n",
    "                    logger_nodes.add(ln)\n",
    "    return list(logger_nodes)\n",
    "\n",
    "def has_cycle(graph):\n",
    "    def dfs(node, visited, rec_stack):\n",
    "        if node not in visited:\n",
    "            # Mark the current node as visited and add to the recursion stack\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            # Check all the nodes this node is connected to\n",
    "            for neighbor_info in graph.get(node, []):\n",
    "                neighbor = neighbor_info[0]\n",
    "                # If the neighbor is not visited, do a recursive DFS call\n",
    "                if neighbor not in visited and dfs(neighbor, visited, rec_stack):\n",
    "                    return True\n",
    "                # If the neighbor is already in the recursion stack, it's a cycle\n",
    "                elif neighbor in rec_stack:\n",
    "                    return True\n",
    "            rec_stack.remove(node)\n",
    "        return False\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "    # Check for cycles starting from each node in the graph\n",
    "    for node in graph.keys():\n",
    "        if node not in visited:\n",
    "            if dfs(node, visited, rec_stack):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "'''\n",
    "Making the trace packet:\n",
    "trace_packet = [t_node_calls_dict, t_data_ops_dict, t_ini_node, t_ini_node_type]\n",
    "t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "def add_if_not_present(array, value):\n",
    "    if value not in array:\n",
    "        array.append(value)\n",
    "    return array\n",
    "trace_details_data = pkl_to_dict('node_and_trace_details/new_trace_details_data.pkl')\n",
    "node_split_output = json.load(open(f'./enrichment_runs/{workload_name}/node_split_output.json'))\n",
    "# node_split_output = json.load(open(f'./enrichment_runs/as_dmix1_2/node_split_output.json')) ################# AS EXP\n",
    "all_trace_packets = {}\n",
    "cycle_ctr = 0\n",
    "unique_nodes_check = []\n",
    "for tid in traces_dict:\n",
    "    t_node_calls_dict = gen_node_calls_dict(traces_dict[tid], async_sync_ratio)\n",
    "    t_data_ops_dict = {} # key: data op id, value: data op packet\n",
    "    for t_node in t_node_calls_dict:\n",
    "        for idx in range(len(t_node_calls_dict[t_node])):# Why is it not entering the if loop?\n",
    "            dm_node = t_node_calls_dict[t_node][idx][0]\n",
    "            if node_dets[dm_node][1] == 'db': # ie if dm node is a sf node\n",
    "                # Select a data op id from the data ops dict and pop it\n",
    "                if len(overall_data_ops[dm_node]) == 0:\n",
    "                    print(\"Error: No data ops for sf node\", dm_node)\n",
    "                    break\n",
    "                # Select a data op id from the data ops dict and pop it\n",
    "                op_id, op_packet = get_pop_first_dict_item(overall_data_ops[dm_node])\n",
    "                t_node_calls_dict[t_node][idx][1] = op_id\n",
    "                t_data_ops_dict[op_id] = op_packet\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(traces_dict[tid])\n",
    "    t_ini_node, trace_depth = find_inode_and_graph_depth(G) # getting initial node\n",
    "    t_ini_node_type = get_node_type(t_ini_node, node_split_output)\n",
    "    t_node_calls_dict = remove_self_node_calls(t_node_calls_dict) # Remove self calls in node_calls_dict\n",
    "    # Get log nodes for this request call graph\n",
    "    t_logger_nodes = get_logger_nodes_for_request_call_graph(t_node_calls_dict)\n",
    "    if has_cycle(t_node_calls_dict):\n",
    "        cycle_ctr += 1\n",
    "        continue\n",
    "\n",
    "    trace_packet = {\"tid\": tid, \"node_calls_dict\": t_node_calls_dict, \"data_ops_dict\": t_data_ops_dict,\\\n",
    "                     \"initial_node\": t_ini_node, \"initial_node_type\": t_ini_node_type, \"logger_nodes\": t_logger_nodes}\n",
    "    if has_cycle(t_node_calls_dict):\n",
    "        cycle_ctr += 1\n",
    "    all_trace_packets[tid] = trace_packet\n",
    "print(\"Cycle Ctr: \", cycle_ctr)\n",
    "print(\"Num Unique nodes: \", len(unique_nodes_check))\n",
    "save_dict_as_json(all_trace_packets, f'enrichment_runs/{workload_name}/all_trace_packets')\n",
    "# save_dict_as_json(all_trace_packets, f'enrichment_runs/as_dmix1_2/all_trace_packets') ################# AS EXP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_inodes_cd = {}\n",
    "traces_dict = pkl_to_dict('traces/exp_544nodes_100ktraces.pkl')\n",
    "for tid, e_list in traces_dict.items():\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(e_list)\n",
    "    initial_node, trace_depth = find_inode_and_graph_depth(G)\n",
    "    if tid not in tid_inodes_cd:\n",
    "        tid_inodes_cd[tid] = [initial_node, trace_depth]\n",
    "save_dict_as_json(tid_inodes_cd, f'node_and_trace_details/544_100k_inode_cd_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "traces_dict = pkl_to_dict('traces/exp_500nodes_100ktraces.pkl')\n",
    "unique_nodes = []\n",
    "for tid, e_list in traces_dict.items():\n",
    "    for e in e_list:\n",
    "        if e[0] not in unique_nodes:\n",
    "            unique_nodes.append(e[0])\n",
    "        if e[1] not in unique_nodes:\n",
    "            unique_nodes.append(e[1])\n",
    "print(len(unique_nodes))\n",
    "save_dict_as_json(unique_nodes, f'node_and_trace_details/500_100k_unique_nodes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from the file '{file_path}'.\")\n",
    "\n",
    "# nso = read_json_file(\"enrichment_runs/dmix1_pg_heavy/node_split_output.json\")\n",
    "# atp = read_json_file(\"enrichment_runs/dmix1_pg_heavy/all_trace_packets.json\")\n",
    "# tdd = pkl_to_dict(\"deployment_files/mewbie_client/new_trace_details_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n7006', 'n812', 'n5971', 'n6076', 'n785', 'n2769', 'n8290', 'n1357', 'n6650', 'n3291', 'n2587', 'n945', 'n6141', 'n7475', 'n7547', 'n6212', 'n7924', 'n5500', 'n1410', 'n6342', 'n1765', 'n3820', 'n7356', 'n5871', 'n2663', 'n1200', 'n1436', 'n6046', 'n4912', 'n2868', 'n4578', 'n4850', 'n728', 'n8132', 'n668', 'n2838', 'n165', 'n2409', 'n7016', 'n8030', 'n895', 'n4800', 'n6255', 'n1914', 'n7642', 'n3392', 'n263', 'n423', 'n341', 'n7200', 'n7361', 'n8307', 'n3826', 'n1220', 'n4558', 'n2453', 'n158', 'n9', 'n1581', 'n7796', 'n4376', 'n3441', 'n7994', 'n7206', 'n5775', 'n7567', 'n5155', 'n6134', 'n4748', 'n1513', 'n1011', 'n2756', 'n3035', 'n4690', 'n2996', 'n7421', 'n2158', 'n6096', 'n6286', 'n2291', 'n7510', 'n995', 'n7549', 'n5510', 'n1439', 'n776', 'n7370', 'n572', 'n3888', 'n1860', 'n7595', 'n1562', 'n5015', 'n2685', 'n5447', 'n8221', 'n5259', 'n4610', 'n4592', 'n2134', 'n6693', 'n4923', 'n6109', 'n7900', 'n942', 'n3827', 'n8167', 'n1964', 'n2826', 'n366', 'n7045', 'n4715', 'n5856', 'n4260', 'n7070', 'n3703', 'n5182', 'n7918', 'n3432', 'n3395', 'n5559', 'n6068', 'n6042', 'n4864', 'n2819', 'n486', 'n2739', 'n2526', 'n2273', 'n896', 'n1875', 'n4223', 'n2502', 'n7482', 'n2888', 'n7144', 'n4294', 'n2857', 'n2083', 'n5703', 'n4408', 'n1985', 'n3727', 'n309', 'n2289', 'n3754', 'n7092', 'n7352', 'n6697', 'n2068', 'n2882', 'n1076', 'n2436', 'n2484', 'n4162', 'n2609', 'n5735', 'n2439', 'n4202', 'n5095', 'n2172', 'n1748', 'n2029', 'n6779', 'n3247', 'n2119', 'n7455', 'n1662', 'n1725', 'n1604', 'n2977', 'n7266', 'n2630', 'n6973', 'n8261', 'n2003', 'n3779', 'n5986', 'n2309', 'n5171', 'n3756', 'n4909', 'n6678', 'n6486', 'n8176', 'n3127', 'n272', 'n4904', 'n704', 'n5402', 'n2415', 'n5912', 'n5406', 'n4310', 'n5829', 'n1990', 'n5081', 'n6952', 'n7203', 'n3941', 'n7964', 'n6689', 'n3907'}\n"
     ]
    }
   ],
   "source": [
    "# third_elements = [value[2] for value in tdd.values() if len(value) > 2]\n",
    "# next(iter(atp.items()))\n",
    "def get_all_logger_nodes(data):\n",
    "    logger_nodes = []\n",
    "    # Iterate over the dictionary and extract logger_nodes\n",
    "    for val in data.values():\n",
    "        # print(val['logger_nodes'])\n",
    "        if len(val['logger_nodes']) > 0:\n",
    "            for ln in val['logger_nodes']:\n",
    "                logger_nodes.append(ln)\n",
    "    return logger_nodes\n",
    "all_logger_nodes = get_all_logger_nodes(atp)\n",
    "print(set(all_logger_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n1765', 354503), ('n2134', 210271), ('n4376', 166338), ('n2977', 138532), ('n942', 65478), ('n4202', 56730), ('n5015', 39773), ('n2436', 36547), ('n6952', 36086), ('n6286', 35163)]\n",
      "['n1765', 'n2134', 'n4376', 'n2977', 'n942', 'n4202', 'n5015', 'n2436', 'n6952', 'n6286']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "traces_dict = pkl_to_dict('traces/exp_497nodes_100ktraces.pkl')\n",
    "nso_nlist = []\n",
    "atp_nlist = []\n",
    "def extract_unique_nodes(data):\n",
    "    unique_nodes = set()  # Use a set to keep unique values\n",
    "    \n",
    "    # Iterate over both sf_split and sl_split keys\n",
    "    for split_key in [\"sf_split\", \"sl_split\"]:\n",
    "        if split_key in data:\n",
    "            # Iterate over each nested dictionary\n",
    "            for service in data[split_key].values():\n",
    "                # Add nodes_list items to the set\n",
    "                nodes = service.get(\"nodes_list\", [])\n",
    "                unique_nodes.update(nodes)\n",
    "    \n",
    "    return list(unique_nodes)\n",
    "nso_nlist = extract_unique_nodes(nso)\n",
    "l_list = []\n",
    "for tid in traces_dict:\n",
    "    edges_list = traces_dict[tid]\n",
    "    for edge in edges_list:\n",
    "        l_list.append(edge[0])\n",
    "        l_list.append(edge[1])\n",
    "        # if edge[0] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[0])\n",
    "        # if edge[1] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[1])\n",
    "\n",
    "# print(len(set(nso_nlist)))\n",
    "# print(len(set(atp_nlist)))\n",
    "# print(set(nso_nlist) == set(atp_nlist))\n",
    "# print(set(nso_nlist) == set(unique_nodes_check))\n",
    "\n",
    "top_five = Counter(l_list).most_common(10)\n",
    "top_10 = [item for item, count in Counter(l_list).most_common(10)]\n",
    "print(top_five)\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync Calls: 264365\n",
      "Async Calls: 263939\n",
      "Top 5 most frequent first elements when the second element is not -1:\n",
      "Element n4037 appears 49168 times\n",
      "Element n7049 appears 33921 times\n",
      "Element n3882 appears 22988 times\n",
      "Element n2127 appears 22224 times\n",
      "Element n6572 appears 17279 times\n",
      "Element n2405 appears 14588 times\n",
      "Element n7709 appears 14000 times\n",
      "Element n988 appears 8651 times\n",
      "Element n5507 appears 7702 times\n",
      "Element n2915 appears 7317 times\n",
      "Element n3428 appears 6231 times\n",
      "Element n5223 appears 5221 times\n",
      "Element n2025 appears 3413 times\n",
      "Element n530 appears 3221 times\n",
      "Element n8006 appears 3168 times\n",
      "Element n870 appears 2597 times\n",
      "Element n2382 appears 2089 times\n",
      "Element n6578 appears 1701 times\n",
      "Element n2154 appears 1539 times\n",
      "Element n7028 appears 879 times\n"
     ]
    }
   ],
   "source": [
    "atp = read_json_file(\"enrichment_runs/dmix1_pg_heavy/all_trace_packets.json\")\n",
    "node_counter = Counter()\n",
    "\n",
    "# Iterate over each item in the data dictionary\n",
    "sync_call_ctr = 0\n",
    "async_call_ctr = 0\n",
    "for item in atp.values():\n",
    "    node_calls_dict = item.get(\"node_calls_dict\", {})\n",
    "    # Collect the first elements when the second element is not -1\n",
    "    for calls in node_calls_dict.values():\n",
    "        for call in calls:\n",
    "            if call[2] == 1:\n",
    "                async_call_ctr += 1\n",
    "            else:\n",
    "                sync_call_ctr += 1\n",
    "    first_elements = [\n",
    "        call[0]\n",
    "        for calls in node_calls_dict.values()\n",
    "        for call in calls\n",
    "        if call[1] != -1\n",
    "    ]\n",
    "    # Update the counter with the collected elements\n",
    "    node_counter.update(first_elements)\n",
    "\n",
    "# Get the top 5 most common first elements\n",
    "top_5 = node_counter.most_common(20)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Sync Calls: {sync_call_ctr}\")\n",
    "print(f\"Async Calls: {async_call_ctr}\")\n",
    "print(\"Top 5 most frequent first elements when the second element is not -1:\")\n",
    "for element, count in top_5:\n",
    "    print(f\"Element {element} appears {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
