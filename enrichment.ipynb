{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs: traces_dict, node_details_dict and trace_details_dict\n",
    "# Node details dict= nid: [nis, type]\n",
    "### Config file: DB split and SLtype split\n",
    "### Outputs: updated_node_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dict(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        T_prime = pickle.load(pkl_file)\n",
    "    return T_prime\n",
    "\n",
    "def save_dict_as_pkl(traces_dict, file_name):\n",
    "    with open(file_name+'.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(traces_dict, pkl_file)\n",
    "\n",
    "def save_dict_as_json(traces_dict, file_name):\n",
    "    with open(file_name+'.json', 'w') as json_file:\n",
    "        json.dump(traces_dict, json_file)\n",
    "\n",
    "def read_yaml(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from the file '{file_path}'.\")\n",
    "\n",
    "def build_digraph_from_tracesdict(traces_dict):\n",
    "\n",
    "    full_graph_edge_list = []\n",
    "    for edge_list in traces_dict.values():\n",
    "        full_graph_edge_list.extend(edge_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(full_graph_edge_list)\n",
    "\n",
    "    return G\n",
    "\n",
    "def prune_node_details(traces_dict, node_dets):\n",
    "    nodes_from_traces = []\n",
    "    for _, e_list in traces_dict.items():\n",
    "        for e in e_list:\n",
    "            if e[0] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[0])\n",
    "            if e[1] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[1])\n",
    "    pruned_node_dets = {node: details for node, details\\\n",
    "                         in node_dets.items() if node in nodes_from_traces}\n",
    "\n",
    "    return pruned_node_dets\n",
    "\n",
    "def calc_graph_depth(G, initial_node):\n",
    "    def dfs(node, visited, stack):\n",
    "        # Using stack to avoid cycles\n",
    "        if node in stack:\n",
    "            return 0\n",
    "        if node in visited:\n",
    "            return visited[node]\n",
    "        stack.add(node)\n",
    "        max_depth = 0\n",
    "        for neighbour in G.successors(node):\n",
    "            depth = dfs(neighbour, visited, stack)\n",
    "            max_depth = max(max_depth, depth)\n",
    "        stack.remove(node)\n",
    "        visited[node] = max_depth + 1\n",
    "        \n",
    "        return visited[node]\n",
    "    \n",
    "    visited = {}\n",
    "    stack = set()\n",
    "\n",
    "    return dfs(initial_node, visited, stack)\n",
    "def find_inode_and_graph_depth(G):\n",
    "    max_depth = -1\n",
    "    node_with_max_depth = ''\n",
    "\n",
    "    for node in G.nodes():\n",
    "        depth = calc_graph_depth(G, node)\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "            node_with_max_depth = node\n",
    "    return node_with_max_depth, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes in node_dets:  498\n",
      "Number of SF nodes in trace graph: 247\n",
      "Number of SL nodes in trace graph: 251\n",
      "Database split Input: [['MongoDB', 40], ['Redis', 20], ['Postgres', 40]]\n",
      "Database split output: [['MongoDB', 99], ['Redis', 49], ['Postgres', 99]]\n",
      "Nodes Split Output: {'sf_split': {'MongoDB': {'count': 99, 'nodes_list': ['n134', 'n2474', 'n7768', 'n1514', 'n9359', 'n2607', 'n4162', 'n2160', 'n3253', 'n5223', 'n750', 'n1156', 'n8728', 'n9555', 'n3501', 'n2595', 'n8429', 'n7387', 'n8189', 'n8215', 'n8838', 'n2016', 'n9108', 'n5268', 'n2856', 'n1892', 'n7449', 'n4576', 'n6724', 'n1296', 'n9445', 'n9092', 'n785', 'n4987', 'n3578', 'n4918', 'n2292', 'n9367', 'n188', 'n141', 'n4534', 'n7841', 'n7447', 'n848', 'n7765', 'n2048', 'n7324', 'n8555', 'n9365', 'n4271', 'n4141', 'n41', 'n2603', 'n7411', 'n7222', 'n728', 'n1176', 'n7726', 'n8589', 'n1589', 'n6827', 'n6513', 'n6002', 'n1262', 'n4835', 'n821', 'n45', 'n2479', 'n2028', 'n2683', 'n4594', 'n435', 'n8170', 'n4160', 'n6664', 'n8405', 'n2398', 'n5963', 'n3848', 'n4191', 'n4340', 'n4523', 'n7080', 'n8407', 'n3594', 'n103', 'n7483', 'n4815', 'n2719', 'n5995', 'n5847', 'n3444', 'n9281', 'n4828', 'n3931', 'n7178', 'n6166', 'n7554', 'n9291']}, 'Redis': {'count': 49, 'nodes_list': ['n2267', 'n1553', 'n8211', 'n8436', 'n1898', 'n5302', 'n7265', 'n1116', 'n7220', 'n4349', 'n8857', 'n1193', 'n7139', 'n7681', 'n7692', 'n3850', 'n1871', 'n5445', 'n1634', 'n1644', 'n7960', 'n5452', 'n6799', 'n8408', 'n744', 'n1925', 'n2617', 'n9123', 'n483', 'n8755', 'n8175', 'n6925', 'n8487', 'n2431', 'n6670', 'n8905', 'n4133', 'n6181', 'n8844', 'n7186', 'n8177', 'n7244', 'n9188', 'n5441', 'n3392', 'n158', 'n4812', 'n4990', 'n6442']}, 'Postgres': {'count': 99, 'nodes_list': ['n4797', 'n3526', 'n8587', 'n3184', 'n3411', 'n4892', 'n5431', 'n4498', 'n2912', 'n1826', 'n7245', 'n9276', 'n384', 'n988', 'n9142', 'n532', 'n2812', 'n8683', 'n9136', 'n4272', 'n1242', 'n7964', 'n8041', 'n5845', 'n1232', 'n220', 'n5406', 'n6805', 'n197', 'n6396', 'n4606', 'n6909', 'n6808', 'n5451', 'n6651', 'n6167', 'n6360', 'n4789', 'n3419', 'n6006', 'n6104', 'n9289', 'n5586', 'n6574', 'n7885', 'n5777', 'n5310', 'n3260', 'n1368', 'n6074', 'n7597', 'n4914', 'n5790', 'n9560', 'n2357', 'n2190', 'n4674', 'n4084', 'n2184', 'n3933', 'n5064', 'n2143', 'n1150', 'n1082', 'n8931', 'n5278', 'n7025', 'n6278', 'n4221', 'n2711', 'n7209', 'n1962', 'n5414', 'n3854', 'n7948', 'n4132', 'n5190', 'n8699', 'n1196', 'n8782', 'n6739', 'n5171', 'n3462', 'n9619', 'n7986', 'n4034', 'n3123', 'n1935', 'n1250', 'n3098', 'n6439', 'n6656', 'n2375', 'n6257', 'n2221', 'n3401', 'n5899', 'n1253', 'n9013']}}, 'sl_split': {'Python': {'count': 251, 'nodes_list': ['n6089', 'n2133', 'n8223', 'n8944', 'n7661', 'n3236', 'n493', 'n8572', 'n9218', 'n5879', 'n4386', 'n6295', 'n2562', 'n1726', 'n3541', 'n8678', 'n3863', 'n3852', 'n7030', 'n6200', 'n5093', 'n1797', 'n1047', 'n1168', 'n2282', 'n4273', 'n2574', 'n8552', 'n2299', 'n3537', 'n3305', 'n9369', 'n4843', 'n8981', 'n4452', 'n2993', 'n6846', 'n5716', 'n4478', 'n6290', 'n4371', 'n1709', 'n4648', 'n2669', 'n3326', 'n6992', 'n2005', 'n4871', 'n8043', 'n6693', 'n6663', 'n2988', 'n566', 'n4157', 'n2480', 'n2576', 'n3992', 'n1736', 'n504', 'n4467', 'n2156', 'n6557', 'n5920', 'n1601', 'n4174', 'n3300', 'n1149', 'n5594', 'n6605', 'n533', 'n1636', 'n4279', 'n254', 'n4182', 'n3675', 'n3932', 'n6969', 'n7810', 'n3646', 'n2806', 'n7019', 'n6972', 'n4138', 'n2891', 'n8119', 'n3319', 'n6537', 'n9447', 'n4526', 'n9605', 'n5962', 'n1401', 'n8197', 'n7561', 'n1390', 'n5930', 'n5888', 'n8077', 'n4179', 'n3652', 'n2237', 'n9392', 'n4333', 'n7326', 'n1630', 'n3074', 'n4202', 'n2738', 'n5659', 'n9474', 'n2782', 'n3605', 'n8188', 'n2800', 'n3201', 'n1489', 'n9384', 'n8560', 'n3953', 'n7590', 'n7990', 'n3909', 'n5', 'n1716', 'n175', 'n5535', 'n3176', 'n5091', 'n9196', 'n8612', 'n5117', 'n40', 'n3820', 'n4110', 'n9315', 'n6809', 'n1024', 'n2650', 'n2870', 'n3739', 'n1840', 'n8067', 'n5551', 'n52', 'n6109', 'n6704', 'n1266', 'n5541', 'n4555', 'n9166', 'n6390', 'n3339', 'n3175', 'n6650', 'n4593', 'n652', 'n9630', 'n6403', 'n8906', 'n9579', 'n5641', 'n7331', 'n2468', 'n4381', 'n8090', 'n4739', 'n1835', 'n3948', 'n4140', 'n3178', 'n9517', 'n6674', 'n8097', 'n8241', 'n3287', 'n9368', 'n1069', 'n8316', 'n2836', 'n9567', 'n7219', 'n208', 'n5128', 'n5311', 'n9518', 'n1964', 'n5133', 'n5436', 'n4275', 'n2346', 'n7309', 'n4348', 'n8375', 'n1574', 'n6787', 'n2146', 'n1396', 'n9280', 'n6766', 'n109', 'n1922', 'n524', 'n3310', 'n721', 'n2741', 'n5070', 'n1551', 'n6654', 'n7547', 'n3762', 'n3205', 'n8933', 'n3573', 'n8983', 'n3818', 'n5602', 'n6579', 'n528', 'n5925', 'n6385', 'n3937', 'n5977', 'n893', 'n323', 'n5386', 'n2351', 'n3949', 'n321', 'n5399', 'n7201', 'n3224', 'n9461', 'n9081', 'n9179', 'n2501', 'n4145', 'n1079', 'n3037', 'n2797', 'n990', 'n1572', 'n2460', 'n3709', 'n8179', 'n5173', 'n4397', 'n9098', 'n7854', 'n770', 'n1590', 'n7332']}}}\n"
     ]
    }
   ],
   "source": [
    "# Read in configs\n",
    "config = read_yaml('enrichment_config.yaml')\n",
    "databases = config['Databases']\n",
    "workload_name = config['ExpWorkloadName']\n",
    "## Datastore mixtures: dmix1_pg_heavy, dmix2_mongo_heavy, dmix3_redis_heavy\n",
    "## Consistency exp: cons_exp (Mongo:Redis:Postgres; 40:40:20)\n",
    "# workload_name = \"cons_exp\"\n",
    "\n",
    "'''\n",
    "NODE ENRICHMENT\n",
    "---------------\n",
    "Input: Traces_dict, Node_details_dict\n",
    "Output: Node split output\n",
    "        node_split_output = {'sf_split': {DB1: {'count': 30, 'nodes_list': [nid1, nid2, ...]}, ...},..}\n",
    "                             'sl_split': ,,}\n",
    "'''\n",
    "\n",
    "traces_dict = pkl_to_dict('traces/final_500nodes_250ktraces.pkl')\n",
    "node_dets = pkl_to_dict('node_and_trace_details/final_node_details_data.pkl') # Node details dict= nid: [nis, SF, DB_name] (or) [nis, SL, SL_type]\n",
    "\n",
    "node_dets = prune_node_details(traces_dict, node_dets)\n",
    "print(\"Num nodes in node_dets: \", len(node_dets))\n",
    "\n",
    "sf_arr = [nid for nid, n_info in node_dets.items() if n_info[1] == \"db\"]\n",
    "sl_arr = [nid for nid, n_info in node_dets.items() if n_info[1] != \"db\"]\n",
    "\n",
    "sf_count = len(sf_arr)\n",
    "print(\"Number of SF nodes in trace graph:\", sf_count)\n",
    "sl_count = len(sl_arr)\n",
    "print(\"Number of SL nodes in trace graph:\", sl_count)\n",
    "total_nodes = sf_count + sl_count\n",
    "\n",
    "db_split_arr = [[db_name, info['percentage']] for db_name, info in databases.items()]# [[DB1, 30%],...]\n",
    "sl_type_split = [['Python', sl_count]]\n",
    "print(\"Database split Input:\", db_split_arr)\n",
    "\n",
    "def percent_to_count(arr, count):\n",
    "    raw_counts = [round(count * (i[1] / 100)) for i in arr]\n",
    "    diff = count - sum(raw_counts)\n",
    "    \n",
    "    # Distribute the difference\n",
    "    idx = 0\n",
    "    while diff != 0:\n",
    "        if diff > 0:\n",
    "            raw_counts[idx] += 1  # Increase by 1 if we need to add\n",
    "            diff -= 1\n",
    "        elif diff < 0:\n",
    "            raw_counts[idx] -= 1  # Decrease by 1 if we need to remove\n",
    "            diff += 1\n",
    "        idx = (idx + 1) % len(raw_counts)\n",
    "    \n",
    "    for idx, i in enumerate(arr):\n",
    "        arr[idx] = [i[0], raw_counts[idx]]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "db_split_arr = percent_to_count(db_split_arr, sf_count) # nid: [nis, SF, DB_name]\n",
    "print(\"Database split output:\", db_split_arr)\n",
    "\n",
    "\n",
    "sf_split_info = {ntype: {\"count\": value, \"nodes_list\": []} for ntype, value in db_split_arr}\n",
    "sl_split_info = {'Python': {\"count\": sl_count, \"nodes_list\": []}}\n",
    "\n",
    "def assign_nodes_to_types(split_arr, sfsl_arr, split_info):\n",
    "    \n",
    "    sfsl_arr_cpy = sfsl_arr.copy()\n",
    "    # Hot Node manual placement\n",
    "    # hot_nodes = [\"n4576\", \"n103\", \"n1082\", \"n744\", \"n9555\", \"n3184\", \"n4835\"] # \"n744\", \"n9555\", \"n3184\", \"n4835\", \"n750\"\n",
    "    # for hot_nid in hot_nodes:\n",
    "    #     if hot_nid in sfsl_arr_cpy:\n",
    "    #         sfsl_arr_cpy.remove(hot_nid)\n",
    "    #         node_dets[hot_nid].append(\"Redis\")\n",
    "    #         split_info[\"Redis\"][\"nodes_list\"].append(hot_nid)\n",
    "    #         for db_type in split_arr:\n",
    "    #             if db_type[0] == \"Redis\":\n",
    "    #                 db_type[1] -= 1\n",
    "\n",
    "    # Assign nodes to db and sl types\n",
    "    for i in split_arr:\n",
    "        ctr = 0\n",
    "        name = i[0] # type name: eg: Mongo, Redis, Relay\n",
    "        for _ in range(i[1]):\n",
    "            ctr += 1\n",
    "            nid = sfsl_arr_cpy.pop(random.randint(0, len(sfsl_arr_cpy) - 1))\n",
    "            node_dets[nid].append(name) # add type to node details\n",
    "            split_info[name][\"nodes_list\"].append(nid) # add node to list of nodes for that type\n",
    "\n",
    "    return node_dets, split_info\n",
    "\n",
    "node_dets, sf_split_info = assign_nodes_to_types(db_split_arr, sf_arr, sf_split_info)\n",
    "node_dets, sl_split_info = assign_nodes_to_types(sl_type_split, sl_arr, sl_split_info)\n",
    "\n",
    "# Saving node split output\n",
    "node_split_output = {'sf_split': sf_split_info, 'sl_split': sl_split_info}\n",
    "print(\"Nodes Split Output:\", node_split_output)\n",
    "save_dict_as_json(node_split_output, f'enrichment_runs/{workload_name}/node_split_output')\n",
    "# save_dict_as_json(node_split_output, f'enrichment_runs/as_dmix1_2/node_split_output') ################# AS EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle Ctr:  23113\n",
      "Trace packets generated and saved.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Object id Enrichment\n",
    "Output: Trace packets.\n",
    "        Trace packets = [t_node_calls_dict, t_data_ops_dict]\n",
    "        t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "        t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "\n",
    "class Wl_config:\n",
    "    \"\"\"\n",
    "    Format: record_count, record_size_dist,\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "    \"\"\"\n",
    "    def __init__(self, record_count, record_size_dist,\\\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed):\n",
    "        self.record_count = record_count\n",
    "        self.record_size_dist = record_size_dist\n",
    "        self.data_access_pattern = data_access_pattern\n",
    "        self.rw_ratio = rw_ratio\n",
    "        self.async_sync_ratio = async_sync_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Setting seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        # Generate object sizes and data access pattern\n",
    "        self.obj_ids_list = np.arange(1, self.record_count + 1)\n",
    "        self.object_sizes_dict = self.generate_object_sizes()\n",
    "        self.probabilities = self.generate_data_access_pattern()\n",
    "\n",
    "    def generate_object_sizes(self):\n",
    "        if self.record_size_dist == 'lognormal':\n",
    "            obj_sizes = np.random.lognormal(mean=np.log(self.record_count), \\\n",
    "                                                 sigma=np.log(self.record_count), \\\n",
    "                                                 size=self.record_count)\n",
    "        elif self.record_size_dist == 'uniform':\n",
    "            obj_sizes = np.random.uniform(low=1, high=self.record_count, size=self.record_count)\n",
    "        else:\n",
    "            raise ValueError('Invalid record size distribution, only lognormal & uniform are allowed for now')\n",
    "        return dict(zip(self.obj_ids_list, obj_sizes))\n",
    "    \n",
    "    def generate_data_access_pattern(self):\n",
    "        if self.data_access_pattern == 'zipfian':\n",
    "            alpha = 1.2\n",
    "            probabilities = np.random.zipf(alpha, len(self.obj_ids_list))\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        elif self.data_access_pattern == 'uniform':\n",
    "            probabilities = np.ones(len(self.obj_ids_list)) / len(self.obj_ids_list)\n",
    "        else:\n",
    "            raise ValueError('Invalid data access pattern, only zipfian & uniform are allowed for now.')\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "# convert edges_list to node_calls_dict format \n",
    "def gen_node_calls_dict(edges_list, async_sync_ratio):\n",
    "    '''\n",
    "    Return: node_calls_dict = Key: dm node, Value: list of [dm node, op_id, async_flag]\n",
    "            (op_id = -1 for SL) (async_flag = 1 for async, 0 for sync)\n",
    "    '''\n",
    "    node_calls_dict = {}\n",
    "    for edge in edges_list:\n",
    "        if edge[0] not in node_calls_dict:\n",
    "            node_calls_dict[edge[0]] = []\n",
    "        async_prob = async_sync_ratio / (1 + async_sync_ratio)\n",
    "        async_flag = 1 if random.random() < async_prob else 0\n",
    "        node_calls_dict[edge[0]].append([edge[1], -1, async_flag]) # [dm node, op_id, async/sync] (-1 for SL) (1/0 for async/sync)\n",
    "    return node_calls_dict\n",
    "\n",
    "def get_pop_first_dict_item(d):\n",
    "    first_key = list(d.keys())[0]\n",
    "    first_item = d.pop(first_key)\n",
    "    return first_key, first_item\n",
    "\n",
    "def get_node_type(node_id, data):\n",
    "    '''\n",
    "    data: node_split_output.json\n",
    "    '''\n",
    "    for split_type, services in data.items():\n",
    "        for service, service_data in services.items():\n",
    "            if node_id in service_data['nodes_list']:\n",
    "                return service\n",
    "\n",
    "def remove_self_node_calls(node_call_dict):\n",
    "    for node, dm_nodes in node_call_dict.items():\n",
    "        node_call_dict[node] = [dm_node for dm_node in dm_nodes if dm_node[0] != node]\n",
    "    return node_call_dict\n",
    "\n",
    "def get_leaf_nodes(node_call_dict):\n",
    "    '''Returns: leaf nodes in a request call graph'''\n",
    "    all_nodes = set(node_call_dict.keys())\n",
    "    called_nodes = set()\n",
    "    for calls in node_call_dict.values():\n",
    "        for call in calls:\n",
    "            called_nodes.add(call[0])\n",
    "    leaf_nodes = called_nodes - all_nodes\n",
    "    return leaf_nodes\n",
    "\n",
    "def get_logger_nodes_for_request_call_graph(node_call_dict):\n",
    "    '''Returns: list of nodes that log for the request call graph\n",
    "                SL leaf nodes and SL node predecessor to SF leaf nodes.\n",
    "    '''\n",
    "    logger_nodes = set()\n",
    "    t_leaf_nodes = get_leaf_nodes(node_call_dict) # find all leaf nodes\n",
    "    for ln in t_leaf_nodes:\n",
    "        for node, calls in node_call_dict.items():\n",
    "            for call in calls:\n",
    "                if call[0] == ln and call[1] != -1: # Leaf SF node\n",
    "                    logger_nodes.add(node)\n",
    "                elif call[0] == ln and call[1] == -1: # Leaf SL node\n",
    "                    logger_nodes.add(ln)\n",
    "    return list(logger_nodes)\n",
    "\n",
    "def gen_sfnode_dataops(sf_node, wl_config, traces_dict, node_dets, node_split_output):\n",
    "    '''\n",
    "    For a given sf node, generate data ops (count total dm calls to sf node)\n",
    "    Return: ops_dict= Key: op_id, Value: op_packet\n",
    "    op_packet = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': op_obj_id,\\\n",
    "                 'db': sf_node_db}\n",
    "    '''\n",
    "    obj_ids_list = wl_config.obj_ids_list\n",
    "    # obj_sizes_dict = wl_config.object_sizes_dict\n",
    "    data_acc_probabilities = wl_config.probabilities\n",
    "    w_prob = wl_config.rw_ratio / (1 + wl_config.rw_ratio)\n",
    "\n",
    "    # sf_node_db = node_dets[sf_node][2]\n",
    "    sf_node_db = None\n",
    "    for db_type, db_info in node_split_output['sf_split'].items():\n",
    "        if sf_node in db_info['nodes_list']:\n",
    "            sf_node_db = db_type\n",
    "            break\n",
    "    if sf_node_db is None:\n",
    "        raise ValueError(f\"Could not determine DB type for node: {sf_node}\")\n",
    "\n",
    "   # find the number of ops to be generated\n",
    "    total_ops = 0\n",
    "    for e_list in traces_dict.values():# count total dm calls to sf node\n",
    "        for e in e_list:\n",
    "            if e[1] == node:\n",
    "                total_ops += 1\n",
    "\n",
    "    # generate ops for sf node\n",
    "    written_obj_ids = set()  \n",
    "    ops_dict = {}   # key: op_id, value: op_packet\n",
    "    for op_id in range(1, total_ops + 1):\n",
    "        # op_type = 'write' if random.random() < w_prob else 'read'\n",
    "        # op_obj_id = np.random.choice(obj_ids_list,\\\n",
    "        #                              p=data_acc_probabilities)# Select by data access pattern\n",
    "        op_obj_id = random.randrange(1, wl_config.record_count + 1)\n",
    "        if op_obj_id not in written_obj_ids:\n",
    "            op_type = 'write'\n",
    "            written_obj_ids.add(op_obj_id)\n",
    "        else:\n",
    "            op_type = 'write' if random.random() < w_prob else 'read'\n",
    "\n",
    "        # op_obj_size = obj_sizes_dict[op_obj_id]\n",
    "        operation = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': f\"key_{op_obj_id}\",\\\n",
    "                      'db': sf_node_db} # op_packet\n",
    "        ops_dict[op_id] = operation\n",
    "    \n",
    "    return ops_dict\n",
    "\n",
    "def has_cycle(graph):\n",
    "    def dfs(node, visited, rec_stack):\n",
    "        if node not in visited:\n",
    "            # Mark the current node as visited and add to the recursion stack\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            # Check all the nodes this node is connected to\n",
    "            for neighbor_info in graph.get(node, []):\n",
    "                neighbor = neighbor_info[0]\n",
    "                # If the neighbor is not visited, do a recursive DFS call\n",
    "                if neighbor not in visited and dfs(neighbor, visited, rec_stack):\n",
    "                    return True\n",
    "                # If the neighbor is already in the recursion stack, it's a cycle\n",
    "                elif neighbor in rec_stack:\n",
    "                    return True\n",
    "            rec_stack.remove(node)\n",
    "        return False\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "    # Check for cycles starting from each node in the graph\n",
    "    for node in graph.keys():\n",
    "        if node not in visited:\n",
    "            if dfs(node, visited, rec_stack):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Reading enrichment config file\n",
    "enrichment_config = read_yaml('enrichment_config.yaml')\n",
    "record_count = enrichment_config['WorkloadConfig']['record_count']\n",
    "record_size_dist = enrichment_config['WorkloadConfig']['record_size_dist']\n",
    "data_access_pattern = enrichment_config['WorkloadConfig']['data_access_pattern']\n",
    "rw_ratio = enrichment_config['WorkloadConfig']['rw_ratio']\n",
    "async_sync_ratio = enrichment_config['WorkloadConfig']['async_sync_ratio']\n",
    "# Format: record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "wl1 = Wl_config(record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed=50) # to be read from config file\n",
    "node_split_output = json.load(open(f'./enrichment_runs/{workload_name}/node_split_output.json'))\n",
    "\n",
    "'''\n",
    "Generate data op packets for each sf node\n",
    "Returns: overall_data_ops = key: sf_node, value: ops_dict {ops_id: op_packet}\n",
    "'''\n",
    "G_agg = build_digraph_from_tracesdict(traces_dict)\n",
    "overall_data_ops = {}   # key: sf_node, value: ops_dict {ops_id: op_packet}\n",
    "check = 0\n",
    "for node in node_dets:\n",
    "    if node in G_agg.nodes() and node_dets[node][1] == 'db':\n",
    "        overall_data_ops[node] = \\\n",
    "            gen_sfnode_dataops(node, wl1, traces_dict, node_dets, node_split_output)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "MAKING THE TRACE PACKET:\n",
    "trace_packet = [t_node_calls_dict, t_data_ops_dict, t_ini_node, t_ini_node_type]\n",
    "t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "def add_if_not_present(array, value):\n",
    "    if value not in array:\n",
    "        array.append(value)\n",
    "    return array\n",
    "\n",
    "def get_sfnode_dbtype(sf_node, node_split_output):\n",
    "    \"\"\"\n",
    "    Returns the DB type (e.g., 'Mongo', 'Redis', 'Postgres') for a given SF node.\n",
    "    Raises an error if the node is not found in the sf_split mapping.\n",
    "    \"\"\"\n",
    "    sf_node_db = None\n",
    "    for db_type, db_info in node_split_output['sf_split'].items():\n",
    "        if sf_node in db_info['nodes_list']:\n",
    "            sf_node_db = db_type\n",
    "            break\n",
    "    if sf_node_db is None:\n",
    "        raise ValueError(f\"Could not determine DB type for node: {sf_node}\")\n",
    "    return sf_node_db\n",
    "\n",
    "all_trace_packets = {}\n",
    "cycle_ctr = 0\n",
    "\n",
    "for tid in traces_dict:\n",
    "    t_node_calls_dict = gen_node_calls_dict(traces_dict[tid], async_sync_ratio)\n",
    "    t_data_ops_dict = {}\n",
    "    local_op_id_counter = 1  # reset per trace\n",
    "\n",
    "    for t_node in t_node_calls_dict:\n",
    "        for idx in range(len(t_node_calls_dict[t_node])):\n",
    "            dm_node, op_id, async_flag = t_node_calls_dict[t_node][idx]\n",
    "\n",
    "            if node_dets[dm_node][1] == 'db':  # SF node\n",
    "                sfnode_dbtype = get_sfnode_dbtype(dm_node, node_split_output)\n",
    "\n",
    "                op_id = local_op_id_counter\n",
    "                local_op_id_counter += 1\n",
    "                t_node_calls_dict[t_node][idx][1] = op_id\n",
    "\n",
    "                op_obj_id = random.randrange(1, wl1.record_count + 1)\n",
    "                op_type = 'write' if random.random() < wl1.rw_ratio / (1 + wl1.rw_ratio) else 'read'\n",
    "\n",
    "                op_packet = {\n",
    "                    'op_id': op_id,\n",
    "                    'op_type': op_type,\n",
    "                    'op_obj_id': f\"key_{op_obj_id}\",\n",
    "                    'db': sfnode_dbtype\n",
    "                }\n",
    "\n",
    "                t_data_ops_dict[op_id] = op_packet\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(traces_dict[tid])\n",
    "    t_ini_node, trace_depth = find_inode_and_graph_depth(G)\n",
    "    t_ini_node_type = get_node_type(t_ini_node, node_split_output)\n",
    "    t_node_calls_dict = remove_self_node_calls(t_node_calls_dict)\n",
    "    t_logger_nodes = get_logger_nodes_for_request_call_graph(t_node_calls_dict)\n",
    "\n",
    "    if has_cycle(t_node_calls_dict):\n",
    "        cycle_ctr += 1\n",
    "        continue\n",
    "\n",
    "    trace_packet = {\n",
    "        \"tid\": tid,\n",
    "        \"node_calls_dict\": t_node_calls_dict,\n",
    "        \"data_ops_dict\": t_data_ops_dict,\n",
    "        \"initial_node\": t_ini_node,\n",
    "        \"initial_node_type\": t_ini_node_type,\n",
    "        \"logger_nodes\": t_logger_nodes\n",
    "    }\n",
    "\n",
    "    all_trace_packets[tid] = trace_packet\n",
    "\n",
    "\n",
    "print(\"Cycle Ctr: \", cycle_ctr)\n",
    "save_dict_as_json(all_trace_packets, f'enrichment_runs/{workload_name}/all_trace_packets')\n",
    "print(\"Trace packets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n7019']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "GENERATE TID TO LOGGER NODES DICT\n",
    "'''\n",
    "exp_name  = \"cons_exp\"  ##################################### CHANGE FOR EACH RUN\n",
    "tpkts = \"enrichment_runs/{}/all_trace_packets.json\".format(exp_name)\n",
    "\n",
    "\n",
    "with open(tpkts, \"r\") as f:\n",
    "    tpkts = json.load(f)\n",
    "\n",
    "# # print one trace packet\n",
    "# first_key = next(iter(tpkts))\n",
    "# tpkt = tpkts[first_key]\n",
    "# print(tpkt)\n",
    "\n",
    "# create a dictionary; key is tid and value is list of logger nodes\n",
    "tid_to_logger_nodes = {}\n",
    "for tpkt in tpkts.values():\n",
    "    tid = tpkt[\"tid\"]\n",
    "    logger_nodes = tpkt[\"logger_nodes\"]\n",
    "    tid_to_logger_nodes[tid] = logger_nodes\n",
    "fk = next(iter(tid_to_logger_nodes))\n",
    "print(tid_to_logger_nodes[fk])\n",
    "# write the dictionary to a json and write to same dir\n",
    "with open(f\"enrichment_runs/{exp_name}/tid_to_logger_nodes.json\", \"w\") as f:\n",
    "    json.dump(tid_to_logger_nodes, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mismatches found\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load files\n",
    "with open('enrichment_runs/cons_exp_rh/node_split_output.json') as f:\n",
    "    node_split = json.load(f)\n",
    "\n",
    "with open('enrichment_runs/cons_exp_rh/all_trace_packets.json') as f:\n",
    "    all_packets = json.load(f)\n",
    "\n",
    "# Build reverse mapping of nodeID -> expected DB type\n",
    "expected_role = {}\n",
    "\n",
    "# Handle SL containers\n",
    "for sl_type, sl_info in node_split['sl_split'].items():\n",
    "    for node_id in sl_info['nodes_list']:\n",
    "        expected_role[node_id] = 'Python'\n",
    "\n",
    "# Handle SF containers (databases)\n",
    "for db_type, db_info in node_split['sf_split'].items():\n",
    "    for node_id in db_info['nodes_list']:\n",
    "        expected_role[node_id] = db_type\n",
    "\n",
    "mismatches = set()\n",
    "\n",
    "for tid, packet in all_packets.items():\n",
    "    data_ops = packet.get('data_ops_dict', {})\n",
    "    node_calls = packet.get('node_calls_dict', {})\n",
    "\n",
    "    for caller_node, calls in node_calls.items():\n",
    "        for callee_info in calls:\n",
    "            callee_node = callee_info[0]\n",
    "            data_op_id = callee_info[1]\n",
    "\n",
    "            # Determine expected type of callee\n",
    "            expected = expected_role.get(callee_node)\n",
    "\n",
    "            if data_op_id != -1:\n",
    "                op_info = data_ops[str(data_op_id)]\n",
    "                actual = op_info['db']\n",
    "            else:\n",
    "                actual = 'Python'\n",
    "\n",
    "            if expected != actual:\n",
    "                mismatches.add(callee_node)\n",
    "if mismatches:\n",
    "    print(f\"Found {len(mismatches)} mismatches\")\n",
    "    print(mismatches)\n",
    "else:\n",
    "    print(\"No mismatches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_inodes_cd = {}\n",
    "traces_dict = pkl_to_dict('traces/exp_544nodes_100ktraces.pkl')\n",
    "for tid, e_list in traces_dict.items():\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(e_list)\n",
    "    initial_node, trace_depth = find_inode_and_graph_depth(G)\n",
    "    if tid not in tid_inodes_cd:\n",
    "        tid_inodes_cd[tid] = [initial_node, trace_depth]\n",
    "save_dict_as_json(tid_inodes_cd, f'node_and_trace_details/544_100k_inode_cd_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tids:  99991\n",
      "Number of tids with no sf:  231\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n7006', 'n812', 'n5971', 'n6076', 'n785', 'n2769', 'n8290', 'n1357', 'n6650', 'n3291', 'n2587', 'n945', 'n6141', 'n7475', 'n7547', 'n6212', 'n7924', 'n5500', 'n1410', 'n6342', 'n1765', 'n3820', 'n7356', 'n5871', 'n2663', 'n1200', 'n1436', 'n6046', 'n4912', 'n2868', 'n4578', 'n4850', 'n728', 'n8132', 'n668', 'n2838', 'n165', 'n2409', 'n7016', 'n8030', 'n895', 'n4800', 'n6255', 'n1914', 'n7642', 'n3392', 'n263', 'n423', 'n341', 'n7200', 'n7361', 'n8307', 'n3826', 'n1220', 'n4558', 'n2453', 'n158', 'n9', 'n1581', 'n7796', 'n4376', 'n3441', 'n7994', 'n7206', 'n5775', 'n7567', 'n5155', 'n6134', 'n4748', 'n1513', 'n1011', 'n2756', 'n3035', 'n4690', 'n2996', 'n7421', 'n2158', 'n6096', 'n6286', 'n2291', 'n7510', 'n995', 'n7549', 'n5510', 'n1439', 'n776', 'n7370', 'n572', 'n3888', 'n1860', 'n7595', 'n1562', 'n5015', 'n2685', 'n5447', 'n8221', 'n5259', 'n4610', 'n4592', 'n2134', 'n6693', 'n4923', 'n6109', 'n7900', 'n942', 'n3827', 'n8167', 'n1964', 'n2826', 'n366', 'n7045', 'n4715', 'n5856', 'n4260', 'n7070', 'n3703', 'n5182', 'n7918', 'n3432', 'n3395', 'n5559', 'n6068', 'n6042', 'n4864', 'n2819', 'n486', 'n2739', 'n2526', 'n2273', 'n896', 'n1875', 'n4223', 'n2502', 'n7482', 'n2888', 'n7144', 'n4294', 'n2857', 'n2083', 'n5703', 'n4408', 'n1985', 'n3727', 'n309', 'n2289', 'n3754', 'n7092', 'n7352', 'n6697', 'n2068', 'n2882', 'n1076', 'n2436', 'n2484', 'n4162', 'n2609', 'n5735', 'n2439', 'n4202', 'n5095', 'n2172', 'n1748', 'n2029', 'n6779', 'n3247', 'n2119', 'n7455', 'n1662', 'n1725', 'n1604', 'n2977', 'n7266', 'n2630', 'n6973', 'n8261', 'n2003', 'n3779', 'n5986', 'n2309', 'n5171', 'n3756', 'n4909', 'n6678', 'n6486', 'n8176', 'n3127', 'n272', 'n4904', 'n704', 'n5402', 'n2415', 'n5912', 'n5406', 'n4310', 'n5829', 'n1990', 'n5081', 'n6952', 'n7203', 'n3941', 'n7964', 'n6689', 'n3907'}\n"
     ]
    }
   ],
   "source": [
    "# third_elements = [value[2] for value in tdd.values() if len(value) > 2]\n",
    "# next(iter(atp.items()))\n",
    "def get_all_logger_nodes(data):\n",
    "    logger_nodes = []\n",
    "    # Iterate over the dictionary and extract logger_nodes\n",
    "    for val in data.values():\n",
    "        # print(val['logger_nodes'])\n",
    "        if len(val['logger_nodes']) > 0:\n",
    "            for ln in val['logger_nodes']:\n",
    "                logger_nodes.append(ln)\n",
    "    return logger_nodes\n",
    "all_logger_nodes = get_all_logger_nodes(atp)\n",
    "print(set(all_logger_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n1765', 354503), ('n2134', 210271), ('n4376', 166338), ('n2977', 138532), ('n942', 65478), ('n4202', 56730), ('n5015', 39773), ('n2436', 36547), ('n6952', 36086), ('n6286', 35163)]\n",
      "['n1765', 'n2134', 'n4376', 'n2977', 'n942', 'n4202', 'n5015', 'n2436', 'n6952', 'n6286']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "traces_dict = pkl_to_dict('traces/exp_497nodes_100ktraces.pkl')\n",
    "nso_nlist = []\n",
    "atp_nlist = []\n",
    "def extract_unique_nodes(data):\n",
    "    unique_nodes = set()  # Use a set to keep unique values\n",
    "    \n",
    "    # Iterate over both sf_split and sl_split keys\n",
    "    for split_key in [\"sf_split\", \"sl_split\"]:\n",
    "        if split_key in data:\n",
    "            # Iterate over each nested dictionary\n",
    "            for service in data[split_key].values():\n",
    "                # Add nodes_list items to the set\n",
    "                nodes = service.get(\"nodes_list\", [])\n",
    "                unique_nodes.update(nodes)\n",
    "    \n",
    "    return list(unique_nodes)\n",
    "nso_nlist = extract_unique_nodes(nso)\n",
    "l_list = []\n",
    "for tid in traces_dict:\n",
    "    edges_list = traces_dict[tid]\n",
    "    for edge in edges_list:\n",
    "        l_list.append(edge[0])\n",
    "        l_list.append(edge[1])\n",
    "        # if edge[0] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[0])\n",
    "        # if edge[1] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[1])\n",
    "\n",
    "# print(len(set(nso_nlist)))\n",
    "# print(len(set(atp_nlist)))\n",
    "# print(set(nso_nlist) == set(atp_nlist))\n",
    "# print(set(nso_nlist) == set(unique_nodes_check))\n",
    "\n",
    "top_five = Counter(l_list).most_common(10)\n",
    "top_10 = [item for item, count in Counter(l_list).most_common(10)]\n",
    "print(top_five)\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync Calls: 874255\n",
      "Async Calls: 874786\n",
      "Top 5 most frequent first elements when the second element is not -1:\n",
      "Element n4576 appears 78652 times\n",
      "Element n103 appears 45170 times\n",
      "Element n1082 appears 39557 times\n",
      "Element n744 appears 36795 times\n",
      "Element n9555 appears 22790 times\n",
      "Element n3184 appears 15846 times\n",
      "Element n4835 appears 15195 times\n",
      "Element n750 appears 8586 times\n",
      "Element n5451 appears 4151 times\n",
      "Element n8555 appears 3789 times\n",
      "Element n2431 appears 3780 times\n",
      "Element n7765 appears 3596 times\n",
      "Element n4191 appears 2794 times\n",
      "Element n6166 appears 2422 times\n",
      "Element n4162 appears 1972 times\n",
      "Element n4594 appears 1943 times\n",
      "Element n5963 appears 1911 times\n",
      "Element n7483 appears 1828 times\n",
      "Element n3260 appears 1797 times\n",
      "Element n9365 appears 1728 times\n"
     ]
    }
   ],
   "source": [
    "atp = read_json_file(\"enrichment_runs/dmix2_mongo_heavy/all_trace_packets.json\")\n",
    "node_counter = Counter()\n",
    "\n",
    "# Iterate over each item in the data dictionary\n",
    "sync_call_ctr = 0\n",
    "async_call_ctr = 0\n",
    "for item in atp.values():\n",
    "    node_calls_dict = item.get(\"node_calls_dict\", {})\n",
    "    # Collect the first elements when the second element is not -1\n",
    "    for calls in node_calls_dict.values():\n",
    "        for call in calls:\n",
    "            if call[2] == 1:\n",
    "                async_call_ctr += 1\n",
    "            else:\n",
    "                sync_call_ctr += 1\n",
    "    first_elements = [\n",
    "        call[0]\n",
    "        # for calls in node_calls_dict.values()\n",
    "        for call in calls\n",
    "        if call[1] != -1\n",
    "    ]\n",
    "    # Update the counter with the collected elements\n",
    "    node_counter.update(first_elements)\n",
    "\n",
    "# Get the top 5 most common first elements\n",
    "top_5 = node_counter.most_common(20)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Sync Calls: {sync_call_ctr}\")\n",
    "print(f\"Async Calls: {async_call_ctr}\")\n",
    "print(\"Top 5 most frequent first elements when the second element is not -1:\")\n",
    "for element, count in top_5:\n",
    "    print(f\"Element {element} appears {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n7019']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "GENERATE TID TO LOGGER NODES DICT\n",
    "'''\n",
    "exp_name  = \"cons_exp_rh\"  ##################################### CHANGE FOR EACH RUN\n",
    "tpkts = \"enrichment_runs/{}/all_trace_packets.json\".format(exp_name)\n",
    "\n",
    "\n",
    "with open(tpkts, \"r\") as f:\n",
    "    tpkts = json.load(f)\n",
    "\n",
    "# # print one trace packet\n",
    "# first_key = next(iter(tpkts))\n",
    "# tpkt = tpkts[first_key]\n",
    "# print(tpkt)\n",
    "\n",
    "# create a dictionary; key is tid and value is list of logger nodes\n",
    "tid_to_logger_nodes = {}\n",
    "for tpkt in tpkts.values():\n",
    "    tid = tpkt[\"tid\"]\n",
    "    logger_nodes = tpkt[\"logger_nodes\"]\n",
    "    tid_to_logger_nodes[tid] = logger_nodes\n",
    "fk = next(iter(tid_to_logger_nodes))\n",
    "print(tid_to_logger_nodes[fk])\n",
    "# write the dictionary to a json and write to same dir\n",
    "with open(f\"enrichment_runs/{exp_name}/tid_to_logger_nodes.json\", \"w\") as f:\n",
    "    json.dump(tid_to_logger_nodes, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atp = read_json_file(\"enrichment_runs/dmix1_pg_heavy/all_trace_packets.json\")\n",
    "nso = read_json_file(\"enrichment_runs/dmix1+pg_heavy/node_split_output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key-value pair of atp: 0b51191915919355084855000e1e53: {'tid': '0b51191915919355084855000e1e53', 'node_calls_dict': {'n2146': [['n7019', -1, 1]], 'n7019': [['n103', 1, 1]]}, 'data_ops_dict': {'1': {'op_id': 1, 'op_type': 'write', 'op_obj_id': 'key_437', 'db': 'Postgres'}}, 'initial_node': 'n2146', 'initial_node_type': 'Python', 'logger_nodes': ['n7019']}\n"
     ]
    }
   ],
   "source": [
    "# Print the first key-value pair of atp\n",
    "first_key_atp = next(iter(atp))\n",
    "print(f\"First key-value pair of atp: {first_key_atp}: {atp[first_key_atp]}\")\n",
    "\n",
    "# Print the first key-value pair of nso\n",
    "# first_key_nso = next(iter(nso))\n",
    "# print(f\"First key-value pair of nso: {first_key_nso}: {nso[first_key_nso]}\")\n",
    "\n",
    "# def find_database_type(node_id, node_split_dict):\n",
    "#     nso = node_split_dict['sf_split']\n",
    "#     for db_type, db_info in nso.items():\n",
    "#         # print(db_type, db_info)\n",
    "#         if node_id in db_info['nodes_list']:\n",
    "#             return db_type\n",
    "#     return \"Unknown\"\n",
    "# # Check database type for node 'n355'\n",
    "# node_id = \"n355\"\n",
    "# db_type = find_database_type(node_id, nso)\n",
    "# print(f\"Node {node_id} is of type: {db_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0b524ede15919482083141000f', {'tid': '0b524ede15919482083141000f', 'node_calls_dict': {'n6292': [['n1016', -1, 1]], 'n1016': [['n3611', -1, 0], ['n699', -1, 0], ['n355', 1, 1], ['n1194', 1, 0], ['n5043', -1, 1]]}, 'data_ops_dict': {'1': {'op_id': 1, 'op_type': 'write', 'op_obj_id': 'key_17', 'db': 'MongoDB'}}, 'initial_node': 'n6292', 'initial_node_type': 'Python', 'logger_nodes': ['n1016', 'n699', 'n5043', 'n3611']})\n",
      "('0b5167e115919488533109000f', {'tid': '0b5167e115919488533109000f', 'node_calls_dict': {'n6292': [['n1016', -1, 1]], 'n1016': [['n5043', -1, 1], ['n355', 2, 0], ['n1194', 2, 1], ['n699', -1, 0], ['n3611', -1, 1]]}, 'data_ops_dict': {'2': {'op_id': 2, 'op_type': 'write', 'op_obj_id': 'key_520', 'db': 'MongoDB'}}, 'initial_node': 'n6292', 'initial_node_type': 'Python', 'logger_nodes': ['n1016', 'n699', 'n5043', 'n3611']})\n",
      "('0b5167e115919489048428000f', {'tid': '0b5167e115919489048428000f', 'node_calls_dict': {'n6292': [['n1016', -1, 0]], 'n1016': [['n3611', -1, 0], ['n699', -1, 1], ['n355', 3, 1], ['n1194', 3, 1], ['n5043', -1, 1]]}, 'data_ops_dict': {'3': {'op_id': 3, 'op_type': 'write', 'op_obj_id': 'key_92', 'db': 'MongoDB'}}, 'initial_node': 'n6292', 'initial_node_type': 'Python', 'logger_nodes': ['n1016', 'n699', 'n5043', 'n3611']})\n",
      "('0b5167e115919477246683000f', {'tid': '0b5167e115919477246683000f', 'node_calls_dict': {'n6292': [['n1016', -1, 1]], 'n1016': [['n3611', -1, 0], ['n699', -1, 1], ['n355', 4, 0], ['n1194', 4, 0], ['n5043', -1, 0]]}, 'data_ops_dict': {'4': {'op_id': 4, 'op_type': 'write', 'op_obj_id': 'key_543', 'db': 'MongoDB'}}, 'initial_node': 'n6292', 'initial_node_type': 'Python', 'logger_nodes': ['n1016', 'n699', 'n5043', 'n3611']})\n"
     ]
    }
   ],
   "source": [
    "def find_trace_packets(node_id, tracepackets_dict):\n",
    "    matching_packets = []\n",
    "    for key, value in tracepackets_dict.items():\n",
    "        if node_id in value.get('node_calls_dict', {}).keys() or any(node_id in sublist for sublist in sum(value.get('node_calls_dict', {}).values(), [])):\n",
    "            matching_packets.append((key, value))\n",
    "        if len(matching_packets) >= 4:\n",
    "            break\n",
    "    return matching_packets\n",
    "matching_packets = find_trace_packets(\"n355\", atp)\n",
    "for packet in matching_packets:\n",
    "    print(packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
