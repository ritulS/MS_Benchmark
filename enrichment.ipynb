{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs: traces_dict, node_details_dict and trace_details_dict\n",
    "# Node details dict= nid: [nis, type]\n",
    "### Config file: DB split and SLtype split\n",
    "### Outputs: updated_node_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dict(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        T_prime = pickle.load(pkl_file)\n",
    "    return T_prime\n",
    "\n",
    "def read_yaml(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n",
      "1652\n",
      "[['MongoDB', 699], ['Redis', 699], ['Postgres', 932]]\n",
      "[['Relay', 495], ['High', 330], ['Low', 826]]\n"
     ]
    }
   ],
   "source": [
    "# Read in configs\n",
    "config = read_yaml('enrichment_config.yaml')\n",
    "databases = config['Databases']\n",
    "sl_types = config['SLTypeSplit']\n",
    "\n",
    "db_split_arr = [[db_name, info['percentage']] for db_name, info in databases.items()]# [[DB1, 30%],...]\n",
    "sl_type_split = [[sl_type, info['percentage']] for sl_type, info in sl_types.items()]# [[Relay, 30%],...]\n",
    "\n",
    "# Node details dict= nid: [nis, SF, DB_name] (or) [nis, SL, SL_type]\n",
    "node_dets = pkl_to_dict('node_details_data.pkl')\n",
    "sf_arr = [nid for nid, n_info in node_dets.items() if n_info[1] == \"db\"]\n",
    "sl_arr = [nid for nid, n_info in node_dets.items() if n_info[1] != \"db\"]\n",
    "\n",
    "sf_count = len(sf_arr)\n",
    "sl_count = len(sl_arr)\n",
    "total_nodes = sf_count + sl_count\n",
    "\n",
    "def percent_to_count(arr, count):\n",
    "    for idx, i in enumerate(arr):\n",
    "        name = i[0]\n",
    "        arr[idx] = [name,int(count * (i[1])/100)]\n",
    "    return arr\n",
    "\n",
    "db_split_arr = percent_to_count(db_split_arr, sf_count)\n",
    "sl_type_split = percent_to_count(sl_type_split, sl_count)\n",
    "\n",
    "print(len(sf_arr))\n",
    "print(len(sl_arr))\n",
    "print(db_split_arr)\n",
    "print(sl_type_split)\n",
    "\n",
    "def assign_nodes_to_types(arr, sfsl_arr):\n",
    "    # Assign nodes to db and sl types\n",
    "    for i in arr:\n",
    "        name = i[0]\n",
    "        for _ in range(i[1]):\n",
    "            nid = sfsl_arr.pop(random.randint(0, len(sfsl_arr) - 1))\n",
    "            node_dets[nid].append(name)\n",
    "    return node_dets\n",
    "node_dets = assign_nodes_to_types(db_split_arr, sf_arr)\n",
    "node_dets = assign_nodes_to_types(sl_type_split, sl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3982\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in node_dets.values():\n",
    "    if len(i) != 3:\n",
    "        check.append(i)\n",
    "print(len(check))\n",
    "print(len(node_dets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObject id Enrichment\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Object id Enrichment\n",
    "'''\n",
    "\n",
    "class Wl_config:\n",
    "    def __init__(self, record_count, record_size_dist,\\\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio):\n",
    "        self.record_count = record_count\n",
    "        self.record_size_dist = record_size_dist\n",
    "        self.data_access_pattern = data_access_pattern\n",
    "        self.rw_ratio = rw_ratio\n",
    "        self.async_sync_ratio = async_sync_ratio\n",
    "\n",
    "wl1 = Wl_config(100, 'uniform', 'zipfian', 0.5, 0.1) # to be read from config file\n",
    "\n",
    "# convert edges_list to node_calls_dict format\n",
    "def convert_to_node_calls_dict(edges_list):\n",
    "    node_calls_dict = {}\n",
    "    for edge in edges_list:\n",
    "        if edge[0] not in node_calls_dict:\n",
    "            node_calls_dict[edge[0]] = []\n",
    "        node_calls_dict[edge[0]].append(edge[1])\n",
    "    return node_calls_dict\n",
    "\n",
    "# for each trace in traces_dict\n",
    "    # extract sf nodes in trace\n",
    "    # for each sf node in trace, perform enrichment \n",
    "        # compute in-degree of sf node\n",
    "        # generate indeg num of data ops\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
