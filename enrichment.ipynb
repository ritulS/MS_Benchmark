{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs: traces_dict, node_details_dict and trace_details_dict\n",
    "# Node details dict= nid: [nis, type]\n",
    "### Config file: DB split and SLtype split\n",
    "### Outputs: updated_node_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dict(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        T_prime = pickle.load(pkl_file)\n",
    "    return T_prime\n",
    "\n",
    "def read_yaml(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "def build_digraph_from_tracesdict(traces_dict):\n",
    "\n",
    "    full_graph_edge_list = []\n",
    "    for edge_list in traces_dict.values():\n",
    "        full_graph_edge_list.extend(edge_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(full_graph_edge_list)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n",
      "1652\n",
      "[['MongoDB', 699], ['Redis', 699], ['Postgres', 932]]\n",
      "[['Relay', 495], ['High', 330], ['Low', 826]]\n"
     ]
    }
   ],
   "source": [
    "# Read in configs\n",
    "config = read_yaml('enrichment_config.yaml')\n",
    "databases = config['Databases']\n",
    "sl_types = config['SLTypeSplit']\n",
    "\n",
    "db_split_arr = [[db_name, info['percentage']] for db_name, info in databases.items()]# [[DB1, 30%],...]\n",
    "sl_type_split = [[sl_type, info['percentage']] for sl_type, info in sl_types.items()]# [[Relay, 30%],...]\n",
    "\n",
    "# Node details dict= nid: [nis, SF, DB_name] (or) [nis, SL, SL_type]\n",
    "node_dets = pkl_to_dict('node_details_data.pkl')\n",
    "sf_arr = [nid for nid, n_info in node_dets.items() if n_info[1] == \"db\"]\n",
    "sl_arr = [nid for nid, n_info in node_dets.items() if n_info[1] != \"db\"]\n",
    "\n",
    "sf_count = len(sf_arr)\n",
    "sl_count = len(sl_arr)\n",
    "total_nodes = sf_count + sl_count\n",
    "\n",
    "def percent_to_count(arr, count):\n",
    "    for idx, i in enumerate(arr):\n",
    "        name = i[0]\n",
    "        arr[idx] = [name,int(count * (i[1])/100)]\n",
    "    return arr\n",
    "\n",
    "db_split_arr = percent_to_count(db_split_arr, sf_count)\n",
    "sl_type_split = percent_to_count(sl_type_split, sl_count)\n",
    "\n",
    "print(len(sf_arr))\n",
    "print(len(sl_arr))\n",
    "print(db_split_arr)\n",
    "print(sl_type_split)\n",
    "\n",
    "def assign_nodes_to_types(arr, sfsl_arr):\n",
    "    # Assign nodes to db and sl types\n",
    "    for i in arr:\n",
    "        name = i[0]\n",
    "        for _ in range(i[1]):\n",
    "            nid = sfsl_arr.pop(random.randint(0, len(sfsl_arr) - 1))\n",
    "            node_dets[nid].append(name)\n",
    "    return node_dets\n",
    "node_dets = assign_nodes_to_types(db_split_arr, sf_arr)\n",
    "node_dets = assign_nodes_to_types(sl_type_split, sl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3982\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in node_dets.values():\n",
    "    if len(i) != 3:\n",
    "        check.append(i)\n",
    "print(len(check))\n",
    "print(len(node_dets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Object id Enrichment\n",
    "'''\n",
    "\n",
    "class Wl_config:\n",
    "    def __init__(self, record_count, record_size_dist,\\\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed):\n",
    "        self.record_count = record_count\n",
    "        self.record_size_dist = record_size_dist\n",
    "        self.data_access_pattern = data_access_pattern\n",
    "        self.rw_ratio = rw_ratio\n",
    "        self.async_sync_ratio = async_sync_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Setting seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        # Generate object sizes and data access pattern\n",
    "        self.obj_ids_list = np.arange(1, self.record_count + 1)\n",
    "        self.object_sizes_dict = self.generate_object_sizes()\n",
    "        self.probabilities = self.generate_data_access_pattern()\n",
    "\n",
    "    def generate_object_sizes(self):\n",
    "        if self.record_size_dist == 'lognormal':\n",
    "            obj_sizes = np.random.lognormal(mean=np.log(self.record_count), \\\n",
    "                                                 sigma=np.log(self.record_count), \\\n",
    "                                                 size=self.record_count)\n",
    "        elif self.record_size_dist == 'uniform':\n",
    "            obj_sizes = np.random.uniform(low=1, high=self.record_count, size=self.record_count)\n",
    "        else:\n",
    "            raise ValueError('Invalid record size distribution')\n",
    "        return dict(zip(self.obj_ids_list, obj_sizes))\n",
    "    \n",
    "    def generate_data_access_pattern(self):\n",
    "        if self.data_access_pattern == 'zipfian':\n",
    "            alpha = 1.2\n",
    "            probabilities = np.random.zipf(alpha, len(self.obj_ids_list))\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        elif self.data_access_pattern == 'uniform':\n",
    "            probabilities = np.ones(len(self.obj_ids_list)) / len(self.obj_ids_list)\n",
    "        else:\n",
    "            raise ValueError('Invalid data access pattern')\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "def gen_sfnode_dataops(sf_node, G_agg, wl_config, node_dets):\n",
    "    '''\n",
    "    For a given sf node, generate indegree num of data ops\n",
    "    Return: ops_dict= Key: op_id, Value: op_packet\n",
    "    op_packet = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': op_obj_id,\\\n",
    "                 'op_obj_size':op_obj_size,'db': sf_node_db}\n",
    "    '''\n",
    "    obj_ids_list = wl_config.obj_ids_list\n",
    "    obj_sizes_dict = wl_config.object_sizes_dict\n",
    "    data_acc_probabilities = wl_config.probabilities\n",
    "    w_prob = wl_config.rw_ratio / (1 + wl_config.rw_ratio)\n",
    "    obj_count = wl_config.record_count\n",
    "\n",
    "    sf_node_db = node_dets[sf_node][2]\n",
    "    total_ops = G_agg.in_degree(sf_node)# gen indeg num of data ops\n",
    "\n",
    "    ops_dict = {}   # key: op_id, value: op_packet\n",
    "    for op_id in range(1, total_ops + 1):\n",
    "        # print(\"opid\", op_id)\n",
    "        op_type = 'write' if random.random() < w_prob else 'read'\n",
    "        op_obj_id = np.random.choice(obj_ids_list,\\\n",
    "                                     p=data_acc_probabilities)# Select by data access pattern\n",
    "        # print(\"op_obj_id\", op_obj_id)\n",
    "        op_obj_size = obj_sizes_dict[op_obj_id]\n",
    "        operation = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': int(op_obj_id),\\\n",
    "                      'op_obj_size':int(op_obj_size),'db': sf_node_db} # op_packet\n",
    "        ops_dict[op_id] = operation\n",
    "    \n",
    "    return ops_dict\n",
    "\n",
    "# convert edges_list to node_calls_dict format\n",
    "def convert_to_node_calls_dict(edges_list):\n",
    "    node_calls_dict = {}\n",
    "    for edge in edges_list:\n",
    "        if edge[0] not in node_calls_dict:\n",
    "            node_calls_dict[edge[0]] = []\n",
    "        node_calls_dict[edge[0]].append([edge[1], -1]) # [dm node, op_id] (-1 for SL)\n",
    "    return node_calls_dict\n",
    "\n",
    "wl1 = Wl_config(100, 'uniform', 'zipfian', 0.5, 0.5, 50) # to be read from config file\n",
    "\n",
    "traces_dict = pkl_to_dict('traces/500nodes_4500traces.pkl')\n",
    "G_agg = build_digraph_from_tracesdict(traces_dict)\n",
    "\n",
    "# Generate data ops for each sf node\n",
    "overall_data_ops = {}   # key: sf_node, value: ops_dict\n",
    "check = []\n",
    "for node in node_dets:\n",
    "    if node in G_agg.nodes() and node_dets[node][1] == 'db':\n",
    "        overall_data_ops[node] = gen_sfnode_dataops(node, G_agg, wl1, node_dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(overall_data_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No data ops for sf node 845d0363e92434814e1b772015c90c08217a4fe7446b06dbdcf0c62c69963861\n",
      "Error: No data ops for sf node 960f5e6d61e96626a514d7a3fed6d8e81ffebd7d0051b3735d8d342261662e26\n",
      "Error: No data ops for sf node 95a9c24648c4754da55afdd47e854bff99381b430f915769c6370ba196af2fd0\n",
      "Error: No data ops for sf node cc10726ed901dca4bf7f553bc97e1223103f20c3df5bed167bbc794824540870\n",
      "Error: No data ops for sf node a7f9f6e2f660b7646118d88512aea0384de690ebeeda64cedc24c72ba2f6d8f4\n",
      "Error: No data ops for sf node 845d0363e92434814e1b772015c90c08217a4fe7446b06dbdcf0c62c69963861\n",
      "Error: No data ops for sf node a18570bba9ced2ba23026e03bff4f220b336731022c1c399c89ac47d566dce1d\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 1ed1099e4d6163ddbdc510bf5d0f8b695bd81567ccc24cc1c549d994ce67f810\n",
      "Error: No data ops for sf node 37c21dd6d96944eaa178e111ba3eff348a7c0d32fe86d11108a8a1f318af1665\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node cc10726ed901dca4bf7f553bc97e1223103f20c3df5bed167bbc794824540870\n",
      "Error: No data ops for sf node 6302b079a1e0806cf9ba4e8493d740754c591be2b7a93efe2e86fad9da049d56\n",
      "Error: No data ops for sf node 7aab1dda4b377cba081bcf66eeace815528e8f2f12d62bf9c9eccc657d81640e\n",
      "Error: No data ops for sf node 6c489e6dd8ca17efc85582e577773c31bf56e425b3f5ea343c4976df9f440fb8\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 4e08230b34ddd24236c45c7e79a035c798408a54a5086cc339f58d1abffedf60\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 0d9adb9ddee29076ce9d31f39089ca5f3311498341fa5ed7eb7777ea47a7c71d\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node eb7c484af426ac4fb8ffeca5c0a18cb2acb335f98b3c93ddf19803fd3ce2212a\n",
      "Error: No data ops for sf node eb7c484af426ac4fb8ffeca5c0a18cb2acb335f98b3c93ddf19803fd3ce2212a\n",
      "Error: No data ops for sf node eb7c484af426ac4fb8ffeca5c0a18cb2acb335f98b3c93ddf19803fd3ce2212a\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node a18570bba9ced2ba23026e03bff4f220b336731022c1c399c89ac47d566dce1d\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 0c2096e4b7ffb3cb2b2d11c61c77979a6d061f97e7ece6270ac3a4f10e613dbf\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 6c489e6dd8ca17efc85582e577773c31bf56e425b3f5ea343c4976df9f440fb8\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 0cb53dac88445c99646db2f5f55a2523fc58af33144391117ae943d035327b78\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 37c21dd6d96944eaa178e111ba3eff348a7c0d32fe86d11108a8a1f318af1665\n",
      "Error: No data ops for sf node c7e083a01111cb7ff37f4cb5af936be370d75a5d828dfa52a31d1c960d87045a\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node b92603bfa16f30e60a9f436fde3a61bfff9ca7f679545b00de6a24eaa7313aee\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 029a32b5baec7e027344c9256cc31b710f2667a9537f30bfb7ccb1c7aeef8d3e\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 95479fcca664301dbf8b425e97f11c840c5f16e39d7033b2c3fb0b735e1c3043\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 95479fcca664301dbf8b425e97f11c840c5f16e39d7033b2c3fb0b735e1c3043\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node e6c047ee12cea974398dae885f9de7aa37f9df1e6241b2487d47881129ddbdea\n",
      "Error: No data ops for sf node 56a5590783952aeb95e1606ea8b3106f8f86f453904810de26a08745afa3872a\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node 56a5590783952aeb95e1606ea8b3106f8f86f453904810de26a08745afa3872a\n",
      "Error: No data ops for sf node d1b569850f04df209add2bbbe4c8f7b93732b0b5bcf0a4d7b13770e3317fd3d3\n",
      "Error: No data ops for sf node eb7c484af426ac4fb8ffeca5c0a18cb2acb335f98b3c93ddf19803fd3ce2212a\n",
      "Error: No data ops for sf node 0b620cc18ba3881ffd930f41d421bff2d6195296c1e6e2ab6011c81ada671aa8\n",
      "Error: No data ops for sf node 1ed1099e4d6163ddbdc510bf5d0f8b695bd81567ccc24cc1c549d994ce67f810\n",
      "Error: No data ops for sf node 724360e25dd7ff89a8c59f595bdc4cae156134e4ad4cf29b3052553bdf6ca915\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node a0811ee9825082c053f028d6a7c88ceb27211192aeff414b8fc560b2584467d6\n",
      "Error: No data ops for sf node ea1f0bed1f09cce77c6ae0df19ee643f8637a423689c1df4b43c045b56a21329\n",
      "Error: No data ops for sf node cc10726ed901dca4bf7f553bc97e1223103f20c3df5bed167bbc794824540870\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Making the trace packet:\n",
    "trace_packet = [t_node_calls_dict, t_data_ops_dict]\n",
    "'''\n",
    "def get_pop_first_dict_item(d):\n",
    "    first_key = list(d.keys())[0]\n",
    "    first_item = d.pop(first_key)\n",
    "    return first_key, first_item\n",
    "\n",
    "ctr = 0\n",
    "err_ctr = 0\n",
    "all_trace_packets = {}\n",
    "for tid in traces_dict:\n",
    "    t_node_calls_dict = convert_to_node_calls_dict(traces_dict[tid])\n",
    "    t_data_ops_dict = {} # key: data op id, value: data op packet\n",
    "    for t_node in t_node_calls_dict:\n",
    "        for idx, dm_node in enumerate(t_node_calls_dict[t_node]):# why is it not entering the if loop?\n",
    "            \n",
    "            if dm_node[0] in overall_data_ops: # ie if dm node is a sf node\n",
    "                \n",
    "                # select a data op id from the data ops dict and pop it\n",
    "                if len(overall_data_ops[dm_node[0]]) == 0:\n",
    "                    print(\"Error: No data ops for sf node\", dm_node[0])\n",
    "                    err_ctr += 1\n",
    "                    break\n",
    "                ctr += 1\n",
    "                op_id, op_packet = get_pop_first_dict_item(overall_data_ops[dm_node[0]])\n",
    "                \n",
    "                t_node_calls_dict[t_node][idx][1] = op_id\n",
    "                t_data_ops_dict[op_id] = op_packet\n",
    "    trace_packet = [t_node_calls_dict, t_data_ops_dict]\n",
    "    all_trace_packets[tid] = trace_packet\n",
    "\n",
    "# CHECK IF INDEG IS EQUAL TO NUM OF DM CALLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "print(ctr)\n",
    "print(err_ctr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
