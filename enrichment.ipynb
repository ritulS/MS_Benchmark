{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs: traces_dict, node_details_dict and trace_details_dict\n",
    "# Node details dict= nid: [nis, type]\n",
    "### Config file: DB split and SLtype split\n",
    "### Outputs: updated_node_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dict(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        T_prime = pickle.load(pkl_file)\n",
    "    return T_prime\n",
    "\n",
    "def save_dict_as_pkl(traces_dict, file_name):\n",
    "    with open(file_name+'.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(traces_dict, pkl_file)\n",
    "\n",
    "def save_dict_as_json(traces_dict, file_name):\n",
    "    with open(file_name+'.json', 'w') as json_file:\n",
    "        json.dump(traces_dict, json_file)\n",
    "\n",
    "def read_yaml(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "def build_digraph_from_tracesdict(traces_dict):\n",
    "\n",
    "    full_graph_edge_list = []\n",
    "    for edge_list in traces_dict.values():\n",
    "        full_graph_edge_list.extend(edge_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(full_graph_edge_list)\n",
    "\n",
    "    return G\n",
    "\n",
    "def prune_node_details(traces_dict, node_dets):\n",
    "    nodes_from_traces = []\n",
    "    for _, e_list in traces_dict.items():\n",
    "        for e in e_list:\n",
    "            if e[0] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[0])\n",
    "            if e[1] not in nodes_from_traces:\n",
    "                nodes_from_traces.append(e[1])\n",
    "    pruned_node_dets = {node: details for node, details\\\n",
    "                         in node_dets.items() if node in nodes_from_traces}\n",
    "\n",
    "    return pruned_node_dets\n",
    "\n",
    "def calc_graph_depth(G, initial_node):\n",
    "    def dfs(node, visited, stack):\n",
    "        # Using stack to avoid cycles\n",
    "        if node in stack:\n",
    "            return 0\n",
    "        if node in visited:\n",
    "            return visited[node]\n",
    "        stack.add(node)\n",
    "        max_depth = 0\n",
    "        for neighbour in G.successors(node):\n",
    "            depth = dfs(neighbour, visited, stack)\n",
    "            max_depth = max(max_depth, depth)\n",
    "        stack.remove(node)\n",
    "        visited[node] = max_depth + 1\n",
    "        \n",
    "        return visited[node]\n",
    "    \n",
    "    visited = {}\n",
    "    stack = set()\n",
    "\n",
    "    return dfs(initial_node, visited, stack)\n",
    "def find_inode_and_graph_depth(G):\n",
    "    max_depth = -1\n",
    "    node_with_max_depth = ''\n",
    "\n",
    "    for node in G.nodes():\n",
    "        depth = calc_graph_depth(G, node)\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "            node_with_max_depth = node\n",
    "    return node_with_max_depth, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes in node_dets:  4\n",
      "Number of SF nodes in trace graph: 1\n",
      "Number of SL nodes in trace graph: 3\n",
      "Database split Input: [['MongoDB', 20], ['Redis', 20], ['Postgres', 60]]\n",
      "Database split output: [['MongoDB', 0], ['Redis', 0], ['Postgres', 1]]\n"
     ]
    }
   ],
   "source": [
    "# Read in configs\n",
    "config = read_yaml('enrichment_config.yaml')\n",
    "databases = config['Databases']\n",
    "workload_name = config['ExpWorkloadName']\n",
    "\n",
    "'''\n",
    "NODE ENRICHMENT\n",
    "---------------\n",
    "Input: Traces_dict, Node_details_dict\n",
    "Output: Node split output\n",
    "        node_split_output = {'sf_split': {DB1: {'count': 30, 'nodes_list': [nid1, nid2, ...]}, ...},..}\n",
    "                             'sl_split': ,,}\n",
    "'''\n",
    "# Node details dict= nid: [nis, SF, DB_name] (or) [nis, SL, SL_type]\n",
    "traces_dict = pkl_to_dict('traces/exp_495nodes_100ktraces.pkl')\n",
    "selected_keys = ['0b66f86d15919401842041000d5482'] # Uncomment only for testing\n",
    "traces_dict = {key: traces_dict[key] for key in selected_keys if key in traces_dict}\n",
    "\n",
    "node_dets = pkl_to_dict('node_and_trace_details/new_node_details_data.pkl')\n",
    "node_dets = prune_node_details(traces_dict, node_dets)\n",
    "print(\"Num nodes in node_dets: \", len(node_dets))\n",
    "\n",
    "sf_arr = [nid for nid, n_info in node_dets.items() if n_info[1] == \"db\"]\n",
    "sl_arr = [nid for nid, n_info in node_dets.items() if n_info[1] != \"db\"]\n",
    "\n",
    "sf_count = len(sf_arr)\n",
    "print(\"Number of SF nodes in trace graph:\", sf_count)\n",
    "sl_count = len(sl_arr)\n",
    "print(\"Number of SL nodes in trace graph:\", sl_count)\n",
    "total_nodes = sf_count + sl_count\n",
    "\n",
    "db_split_arr = [[db_name, info['percentage']] for db_name, info in databases.items()]# [[DB1, 30%],...]\n",
    "sl_type_split = [['Python', sl_count]]\n",
    "print(\"Database split Input:\", db_split_arr)\n",
    "\n",
    "def percent_to_count(arr, count):\n",
    "    raw_counts = [round(count * (i[1] / 100)) for i in arr]\n",
    "    diff = count - sum(raw_counts)\n",
    "    \n",
    "    # Distribute the difference\n",
    "    idx = 0\n",
    "    while diff != 0:\n",
    "        if diff > 0:\n",
    "            raw_counts[idx] += 1  # Increase by 1 if we need to add\n",
    "            diff -= 1\n",
    "        elif diff < 0:\n",
    "            raw_counts[idx] -= 1  # Decrease by 1 if we need to remove\n",
    "            diff += 1\n",
    "        idx = (idx + 1) % len(raw_counts)\n",
    "    \n",
    "    for idx, i in enumerate(arr):\n",
    "        arr[idx] = [i[0], raw_counts[idx]]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "db_split_arr = percent_to_count(db_split_arr, sf_count) # nid: [nis, SF, DB_name]\n",
    "print(\"Database split output:\", db_split_arr)\n",
    "\n",
    "\n",
    "sf_split_info = {ntype: {\"count\": value, \"nodes_list\": []} for ntype, value in db_split_arr}\n",
    "sl_split_info = {'Python': {\"count\": sl_count, \"nodes_list\": []}}\n",
    "\n",
    "def assign_nodes_to_types(split_arr, sfsl_arr, split_info):\n",
    "    sfsl_arr_cpy = sfsl_arr.copy()\n",
    "    # Assign nodes to db and sl types\n",
    "    for i in split_arr:\n",
    "        ctr = 0\n",
    "        name = i[0] # type name: eg: Mongo, Redis, Relay\n",
    "        for _ in range(i[1]):\n",
    "            ctr += 1\n",
    "            nid = sfsl_arr_cpy.pop(random.randint(0, len(sfsl_arr_cpy) - 1))\n",
    "            node_dets[nid].append(name) # add type to node details\n",
    "            split_info[name][\"nodes_list\"].append(nid) # add node to list of nodes for that type\n",
    "        # print(ctr, name)\n",
    "    return node_dets, split_info\n",
    "\n",
    "node_dets, sf_split_info = assign_nodes_to_types(db_split_arr, sf_arr, sf_split_info)\n",
    "node_dets, sl_split_info = assign_nodes_to_types(sl_type_split, sl_arr, sl_split_info)\n",
    "\n",
    "# Saving node split output\n",
    "node_split_output = {'sf_split': sf_split_info, 'sl_split': sl_split_info}\n",
    "# print(\"Nodes Split Output:\", node_split_output)\n",
    "save_dict_as_json(node_split_output, f'enrichment_runs/{workload_name}/node_split_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b66f86d15919401842041000d5482 1 4\n",
      "0b66f71615919495621493000d0f19 1 4\n",
      "0b66f1e015919610718299000d29d4 1 4\n",
      "0b66f71615919622748806000d0f19 1 4\n",
      "0b66f02915919590863192000d4da0 1 4\n"
     ]
    }
   ],
   "source": [
    "# 0b5218f615919497680352000ed6c1 1 5: Mongo testing\n",
    "\n",
    "## Sl to Sl: 0b51062f15919594922205000eaed6 (2 nodes)\n",
    "## Sl to Db: 0b66f86d15919401842041000d5482 (2 nodes)\n",
    "\n",
    "ticker = 0\n",
    "for tid in traces_dict:\n",
    "    e_list = traces_dict[tid]\n",
    "    sf_needed = 0\n",
    "    t_nodes = []\n",
    "    for e in e_list:\n",
    "        if e[1] not in t_nodes:\n",
    "            t_nodes.append(e[1])\n",
    "        if e[0] not in t_nodes:\n",
    "            t_nodes.append(e[0])\n",
    "        if e[1] in sf_arr:\n",
    "            sf_needed += 1\n",
    "    # if sf_needed == 2 and len(t_nodes) < 35:\n",
    "    #     ticker += 1\n",
    "    #     print(tid, sf_needed, len(t_nodes))\n",
    "    if sf_needed == 1 and len(t_nodes) < 35:\n",
    "        ticker += 1\n",
    "        print(tid, sf_needed, len(t_nodes))\n",
    "        if ticker == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle Ctr:  0\n",
      "Num Unique nodes:  0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Object id Enrichment\n",
    "Output: Trace packets.\n",
    "        Trace packets = [t_node_calls_dict, t_data_ops_dict]\n",
    "        t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "        t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "\n",
    "class Wl_config:\n",
    "    \"\"\"\n",
    "    Format: record_count, record_size_dist,\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "    \"\"\"\n",
    "    def __init__(self, record_count, record_size_dist,\\\n",
    "                 data_access_pattern, rw_ratio, async_sync_ratio, seed):\n",
    "        self.record_count = record_count\n",
    "        self.record_size_dist = record_size_dist\n",
    "        self.data_access_pattern = data_access_pattern\n",
    "        self.rw_ratio = rw_ratio\n",
    "        self.async_sync_ratio = async_sync_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Setting seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        # Generate object sizes and data access pattern\n",
    "        self.obj_ids_list = np.arange(1, self.record_count + 1)\n",
    "        self.object_sizes_dict = self.generate_object_sizes()\n",
    "        self.probabilities = self.generate_data_access_pattern()\n",
    "\n",
    "    def generate_object_sizes(self):\n",
    "        if self.record_size_dist == 'lognormal':\n",
    "            obj_sizes = np.random.lognormal(mean=np.log(self.record_count), \\\n",
    "                                                 sigma=np.log(self.record_count), \\\n",
    "                                                 size=self.record_count)\n",
    "        elif self.record_size_dist == 'uniform':\n",
    "            obj_sizes = np.random.uniform(low=1, high=self.record_count, size=self.record_count)\n",
    "        else:\n",
    "            raise ValueError('Invalid record size distribution, only lognormal & uniform are allowed for now')\n",
    "        return dict(zip(self.obj_ids_list, obj_sizes))\n",
    "    \n",
    "    def generate_data_access_pattern(self):\n",
    "        if self.data_access_pattern == 'zipfian':\n",
    "            alpha = 1.2\n",
    "            probabilities = np.random.zipf(alpha, len(self.obj_ids_list))\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        elif self.data_access_pattern == 'uniform':\n",
    "            probabilities = np.ones(len(self.obj_ids_list)) / len(self.obj_ids_list)\n",
    "        else:\n",
    "            raise ValueError('Invalid data access pattern, only zipfian & uniform are allowed for now.')\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "def gen_sfnode_dataops(sf_node, wl_config, traces_dict, node_dets):\n",
    "    '''\n",
    "    For a given sf node, generate data ops (count total dm calls to sf node)\n",
    "    Return: ops_dict= Key: op_id, Value: op_packet\n",
    "    op_packet = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': op_obj_id,\\\n",
    "                 'db': sf_node_db}\n",
    "    '''\n",
    "    obj_ids_list = wl_config.obj_ids_list\n",
    "    # obj_sizes_dict = wl_config.object_sizes_dict\n",
    "    data_acc_probabilities = wl_config.probabilities\n",
    "    w_prob = wl_config.rw_ratio / (1 + wl_config.rw_ratio)\n",
    "\n",
    "    sf_node_db = node_dets[sf_node][2]\n",
    "\n",
    "   # find the number of ops to be generated\n",
    "    total_ops = 0\n",
    "    for e_list in traces_dict.values():# count total dm calls to sf node\n",
    "        for e in e_list:\n",
    "            if e[1] == node:\n",
    "                total_ops += 1\n",
    "\n",
    "    # generate ops for sf node\n",
    "    ops_dict = {}   # key: op_id, value: op_packet\n",
    "    for op_id in range(1, total_ops + 1):\n",
    "        op_type = 'write' if random.random() < w_prob else 'read'\n",
    "        # op_obj_id = np.random.choice(obj_ids_list,\\\n",
    "        #                              p=data_acc_probabilities)# Select by data access pattern\n",
    "        op_obj_id = random.randrange(1, wl_config.record_count + 1)\n",
    "        # op_obj_size = obj_sizes_dict[op_obj_id]\n",
    "        operation = {'op_id': op_id, 'op_type': op_type, 'op_obj_id': f\"key_{op_obj_id}\",\\\n",
    "                      'db': sf_node_db} # op_packet\n",
    "        ops_dict[op_id] = operation\n",
    "    \n",
    "    return ops_dict\n",
    "\n",
    "\n",
    "# convert edges_list to node_calls_dict format \n",
    "def gen_node_calls_dict(edges_list, async_sync_ratio):\n",
    "    '''\n",
    "    Return: node_calls_dict = Key: dm node, Value: list of [dm node, op_id, async_flag]\n",
    "            (op_id = -1 for SL) (async_flag = 1 for async, 0 for sync)\n",
    "    '''\n",
    "    node_calls_dict = {}\n",
    "    for edge in edges_list:\n",
    "        if edge[0] not in node_calls_dict:\n",
    "            node_calls_dict[edge[0]] = []\n",
    "        async_prob = async_sync_ratio / (1 + async_sync_ratio)\n",
    "        async_flag = 1 if random.random() < async_prob else 0\n",
    "        node_calls_dict[edge[0]].append([edge[1], -1, async_flag]) # [dm node, op_id, async/sync] (-1 for SL) (1/0 for async/sync)\n",
    "    return node_calls_dict\n",
    "\n",
    "\n",
    "# Reading enrichment config file\n",
    "enrichment_config = read_yaml('enrichment_config.yaml')\n",
    "record_count = enrichment_config['WorkloadConfig']['record_count']\n",
    "record_size_dist = enrichment_config['WorkloadConfig']['record_size_dist']\n",
    "data_access_pattern = enrichment_config['WorkloadConfig']['data_access_pattern']\n",
    "rw_ratio = enrichment_config['WorkloadConfig']['rw_ratio']\n",
    "async_sync_ratio = enrichment_config['WorkloadConfig']['async_sync_ratio']\n",
    "# Format: record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed\n",
    "wl1 = Wl_config(record_count, record_size_dist, data_access_pattern, rw_ratio, async_sync_ratio, seed=50) # to be read from config file\n",
    "\n",
    "# Generate data op packets for each sf node\n",
    "G_agg = build_digraph_from_tracesdict(traces_dict)\n",
    "overall_data_ops = {}   # key: sf_node, value: ops_dict\n",
    "check = 0\n",
    "for node in node_dets:\n",
    "    if node in G_agg.nodes() and node_dets[node][1] == 'db':\n",
    "        overall_data_ops[node] = \\\n",
    "            gen_sfnode_dataops(node, wl1, traces_dict, node_dets)\n",
    "\n",
    "def get_pop_first_dict_item(d):\n",
    "    first_key = list(d.keys())[0]\n",
    "    first_item = d.pop(first_key)\n",
    "    return first_key, first_item\n",
    "\n",
    "def get_node_type(node_id, data):\n",
    "    '''\n",
    "    data: node_split_output.json\n",
    "    '''\n",
    "    for split_type, services in data.items():\n",
    "        for service, service_data in services.items():\n",
    "            if node_id in service_data['nodes_list']:\n",
    "                return service\n",
    "\n",
    "def remove_self_node_calls(node_call_dict):\n",
    "    for node, dm_nodes in node_call_dict.items():\n",
    "        node_call_dict[node] = [dm_node for dm_node in dm_nodes if dm_node[0] != node]\n",
    "    return node_call_dict\n",
    "\n",
    "def get_leaf_nodes(node_call_dict):\n",
    "    '''Returns: leaf nodes in a request call graph'''\n",
    "    all_nodes = set(node_call_dict.keys())\n",
    "    called_nodes = set()\n",
    "    for calls in node_call_dict.values():\n",
    "        for call in calls:\n",
    "            called_nodes.add(call[0])\n",
    "    leaf_nodes = called_nodes - all_nodes\n",
    "    return leaf_nodes\n",
    "\n",
    "def get_logger_nodes_for_request_call_graph(node_call_dict):\n",
    "    '''Returns: list of nodes that log for the request call graph\n",
    "                SL leaf nodes and SL node predecessor to SF leaf nodes.\n",
    "    '''\n",
    "    logger_nodes = set()\n",
    "    t_leaf_nodes = get_leaf_nodes(node_call_dict) # find all leaf nodes\n",
    "    for ln in t_leaf_nodes:\n",
    "        for node, calls in node_call_dict.items():\n",
    "            for call in calls:\n",
    "                if call[0] == ln and call[1] != -1: # Leaf SF node\n",
    "                    logger_nodes.add(node)\n",
    "                elif call[0] == ln and call[1] == -1: # Leaf SL node\n",
    "                    logger_nodes.add(ln)\n",
    "    return list(logger_nodes)\n",
    "\n",
    "def has_cycle(graph):\n",
    "    def dfs(node, visited, rec_stack):\n",
    "        if node not in visited:\n",
    "            # Mark the current node as visited and add to the recursion stack\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            # Check all the nodes this node is connected to\n",
    "            for neighbor_info in graph.get(node, []):\n",
    "                neighbor = neighbor_info[0]\n",
    "                # If the neighbor is not visited, do a recursive DFS call\n",
    "                if neighbor not in visited and dfs(neighbor, visited, rec_stack):\n",
    "                    return True\n",
    "                # If the neighbor is already in the recursion stack, it's a cycle\n",
    "                elif neighbor in rec_stack:\n",
    "                    return True\n",
    "            rec_stack.remove(node)\n",
    "        return False\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "    # Check for cycles starting from each node in the graph\n",
    "    for node in graph.keys():\n",
    "        if node not in visited:\n",
    "            if dfs(node, visited, rec_stack):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "'''\n",
    "Making the trace packet:\n",
    "trace_packet = [t_node_calls_dict, t_data_ops_dict, t_ini_node, t_ini_node_type]\n",
    "t_node_calls_dict = Key: dm node, Value: list of [dm node, op_id]\n",
    "t_data_ops_dict = Key: data op id, Value: data op packet\n",
    "'''\n",
    "def add_if_not_present(array, value):\n",
    "    if value not in array:\n",
    "        array.append(value)\n",
    "    return array\n",
    "trace_details_data = pkl_to_dict('node_and_trace_details/new_trace_details_data.pkl')\n",
    "node_split_output = json.load(open(f'./enrichment_runs/{workload_name}/node_split_output.json'))\n",
    "all_trace_packets = {}\n",
    "cycle_ctr = 0\n",
    "unique_nodes_check = []\n",
    "for tid in traces_dict:\n",
    "    t_node_calls_dict = gen_node_calls_dict(traces_dict[tid], async_sync_ratio)\n",
    "    t_data_ops_dict = {} # key: data op id, value: data op packet\n",
    "    for t_node in t_node_calls_dict:\n",
    "        for idx in range(len(t_node_calls_dict[t_node])):# Why is it not entering the if loop?\n",
    "            dm_node = t_node_calls_dict[t_node][idx][0]\n",
    "            if node_dets[dm_node][1] == 'db': # ie if dm node is a sf node\n",
    "                # Select a data op id from the data ops dict and pop it\n",
    "                if len(overall_data_ops[dm_node]) == 0:\n",
    "                    print(\"Error: No data ops for sf node\", dm_node)\n",
    "                    break\n",
    "                # Select a data op id from the data ops dict and pop it\n",
    "                op_id, op_packet = get_pop_first_dict_item(overall_data_ops[dm_node])\n",
    "                t_node_calls_dict[t_node][idx][1] = op_id\n",
    "                t_data_ops_dict[op_id] = op_packet\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(traces_dict[tid])\n",
    "    t_ini_node, trace_depth = find_inode_and_graph_depth(G) # getting initial node\n",
    "    t_ini_node_type = get_node_type(t_ini_node, node_split_output)\n",
    "    t_node_calls_dict = remove_self_node_calls(t_node_calls_dict) # Remove self calls in node_calls_dict\n",
    "    # Get log nodes for this request call graph\n",
    "    t_logger_nodes = get_logger_nodes_for_request_call_graph(t_node_calls_dict)\n",
    "    if has_cycle(t_node_calls_dict):\n",
    "        continue\n",
    "\n",
    "    trace_packet = {\"tid\": tid, \"node_calls_dict\": t_node_calls_dict, \"data_ops_dict\": t_data_ops_dict,\\\n",
    "                     \"initial_node\": t_ini_node, \"initial_node_type\": t_ini_node_type, \"logger_nodes\": t_logger_nodes}\n",
    "    if has_cycle(t_node_calls_dict):\n",
    "        cycle_ctr += 1\n",
    "    all_trace_packets[tid] = trace_packet\n",
    "print(\"Cycle Ctr: \", cycle_ctr)\n",
    "print(\"Num Unique nodes: \", len(unique_nodes_check))\n",
    "save_dict_as_json(all_trace_packets, f'enrichment_runs/{workload_name}/all_trace_packets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_inodes_cd = {}\n",
    "traces_dict = pkl_to_dict('traces/exp_495nodes_100ktraces.pkl')\n",
    "for tid, e_list in traces_dict.items():\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(e_list)\n",
    "    initial_node, trace_depth = find_inode_and_graph_depth(G)\n",
    "    if tid not in tid_inodes_cd:\n",
    "        tid_inodes_cd[tid] = [initial_node, trace_depth]\n",
    "save_dict_as_json(tid_inodes_cd, f'node_and_trace_details/495_100k_inode_cd_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_json_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"The file '{file_path}' was not found.\")\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(f\"Error decoding JSON from the file '{file_path}'.\")\n",
    "\n",
    "# nso = read_json_file(\"enrichment_runs/dmix1_pg_heavy/node_split_output.json\")\n",
    "# atp = read_json_file(\"enrichment_runs/dmix1_pg_heavy/all_trace_packets.json\")\n",
    "# tdd = pkl_to_dict(\"deployment_files/mewbie_client/new_trace_details_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n7006', 'n812', 'n5971', 'n6076', 'n785', 'n2769', 'n8290', 'n1357', 'n6650', 'n3291', 'n2587', 'n945', 'n6141', 'n7475', 'n7547', 'n6212', 'n7924', 'n5500', 'n1410', 'n6342', 'n1765', 'n3820', 'n7356', 'n5871', 'n2663', 'n1200', 'n1436', 'n6046', 'n4912', 'n2868', 'n4578', 'n4850', 'n728', 'n8132', 'n668', 'n2838', 'n165', 'n2409', 'n7016', 'n8030', 'n895', 'n4800', 'n6255', 'n1914', 'n7642', 'n3392', 'n263', 'n423', 'n341', 'n7200', 'n7361', 'n8307', 'n3826', 'n1220', 'n4558', 'n2453', 'n158', 'n9', 'n1581', 'n7796', 'n4376', 'n3441', 'n7994', 'n7206', 'n5775', 'n7567', 'n5155', 'n6134', 'n4748', 'n1513', 'n1011', 'n2756', 'n3035', 'n4690', 'n2996', 'n7421', 'n2158', 'n6096', 'n6286', 'n2291', 'n7510', 'n995', 'n7549', 'n5510', 'n1439', 'n776', 'n7370', 'n572', 'n3888', 'n1860', 'n7595', 'n1562', 'n5015', 'n2685', 'n5447', 'n8221', 'n5259', 'n4610', 'n4592', 'n2134', 'n6693', 'n4923', 'n6109', 'n7900', 'n942', 'n3827', 'n8167', 'n1964', 'n2826', 'n366', 'n7045', 'n4715', 'n5856', 'n4260', 'n7070', 'n3703', 'n5182', 'n7918', 'n3432', 'n3395', 'n5559', 'n6068', 'n6042', 'n4864', 'n2819', 'n486', 'n2739', 'n2526', 'n2273', 'n896', 'n1875', 'n4223', 'n2502', 'n7482', 'n2888', 'n7144', 'n4294', 'n2857', 'n2083', 'n5703', 'n4408', 'n1985', 'n3727', 'n309', 'n2289', 'n3754', 'n7092', 'n7352', 'n6697', 'n2068', 'n2882', 'n1076', 'n2436', 'n2484', 'n4162', 'n2609', 'n5735', 'n2439', 'n4202', 'n5095', 'n2172', 'n1748', 'n2029', 'n6779', 'n3247', 'n2119', 'n7455', 'n1662', 'n1725', 'n1604', 'n2977', 'n7266', 'n2630', 'n6973', 'n8261', 'n2003', 'n3779', 'n5986', 'n2309', 'n5171', 'n3756', 'n4909', 'n6678', 'n6486', 'n8176', 'n3127', 'n272', 'n4904', 'n704', 'n5402', 'n2415', 'n5912', 'n5406', 'n4310', 'n5829', 'n1990', 'n5081', 'n6952', 'n7203', 'n3941', 'n7964', 'n6689', 'n3907'}\n"
     ]
    }
   ],
   "source": [
    "# third_elements = [value[2] for value in tdd.values() if len(value) > 2]\n",
    "# next(iter(atp.items()))\n",
    "def get_all_logger_nodes(data):\n",
    "    logger_nodes = []\n",
    "    # Iterate over the dictionary and extract logger_nodes\n",
    "    for val in data.values():\n",
    "        # print(val['logger_nodes'])\n",
    "        if len(val['logger_nodes']) > 0:\n",
    "            for ln in val['logger_nodes']:\n",
    "                logger_nodes.append(ln)\n",
    "    return logger_nodes\n",
    "all_logger_nodes = get_all_logger_nodes(atp)\n",
    "print(set(all_logger_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n1765', 354503), ('n2134', 210271), ('n4376', 166338), ('n2977', 138532), ('n942', 65478), ('n4202', 56730), ('n5015', 39773), ('n2436', 36547), ('n6952', 36086), ('n6286', 35163)]\n",
      "['n1765', 'n2134', 'n4376', 'n2977', 'n942', 'n4202', 'n5015', 'n2436', 'n6952', 'n6286']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "traces_dict = pkl_to_dict('traces/exp_495nodes_100ktraces.pkl')\n",
    "nso_nlist = []\n",
    "atp_nlist = []\n",
    "def extract_unique_nodes(data):\n",
    "    unique_nodes = set()  # Use a set to keep unique values\n",
    "    \n",
    "    # Iterate over both sf_split and sl_split keys\n",
    "    for split_key in [\"sf_split\", \"sl_split\"]:\n",
    "        if split_key in data:\n",
    "            # Iterate over each nested dictionary\n",
    "            for service in data[split_key].values():\n",
    "                # Add nodes_list items to the set\n",
    "                nodes = service.get(\"nodes_list\", [])\n",
    "                unique_nodes.update(nodes)\n",
    "    \n",
    "    return list(unique_nodes)\n",
    "nso_nlist = extract_unique_nodes(nso)\n",
    "l_list = []\n",
    "for tid in traces_dict:\n",
    "    edges_list = traces_dict[tid]\n",
    "    for edge in edges_list:\n",
    "        l_list.append(edge[0])\n",
    "        l_list.append(edge[1])\n",
    "        # if edge[0] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[0])\n",
    "        # if edge[1] not in atp_nlist:\n",
    "        #     atp_nlist.append(edge[1])\n",
    "\n",
    "# print(len(set(nso_nlist)))\n",
    "# print(len(set(atp_nlist)))\n",
    "# print(set(nso_nlist) == set(atp_nlist))\n",
    "# print(set(nso_nlist) == set(unique_nodes_check))\n",
    "\n",
    "top_five = Counter(l_list).most_common(10)\n",
    "top_10 = [item for item, count in Counter(l_list).most_common(10)]\n",
    "print(top_five)\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_run'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml('enrichment_config.yaml')\n",
    "config['ExpWorkloadName']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
